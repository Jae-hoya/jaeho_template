{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Github Repository 기반 Q&A 시스템 구축\n",
    "\n",
    "- Github Repository: https://github.com/langchain-ai/langgraph\n",
    "\n",
    "1. \"Code\" - \"Download ZIP\" 버튼을 눌러 다운로드 받습니다.\n",
    "2. 다운로드 받은 파일을 압축 해제합니다.\n",
    "\n",
    "혹은 아래 명령어로 repository 를 다운로드 받습니다.\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/langchain-ai/langgraph.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH12-RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# 최신 LLM 모델 이름 가져오기\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "print(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "먼저, langgraph 의 다운로드 받은 github repository 의 파일을 로드합니다.\n",
    "\n",
    "여기서 로드하는 파일은 파이썬 파일(.py), 마크다운 파일(.md), 노트북 파일(.ipynb) 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain_text_splitters import Language\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "import utils\n",
    "import os\n",
    "\n",
    "# 프로젝트 루트 경로\n",
    "ROOT_PATH = \"/Users/teddy/Dev/github/langgraph-main\"\n",
    "# ROOT_PATH = r\"C:\\Users\\teddy/Dev/github/langgraph-main\"\n",
    "\n",
    "# 가져올 문서 경로\n",
    "libs_path = ROOT_PATH + \"/libs\"\n",
    "docs_path = ROOT_PATH + \"/docs\"\n",
    "examples_path = ROOT_PATH + \"/examples\"\n",
    "\n",
    "all_repos = [\n",
    "    libs_path,\n",
    "    docs_path,\n",
    "    examples_path,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 각 형식의 문서를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 파일 로드\n",
    "def load_python_files(repos):\n",
    "    documents = []\n",
    "    for repo in repos:\n",
    "        loader = GenericLoader.from_filesystem(\n",
    "            repo,\n",
    "            glob=\"**/*\",\n",
    "            suffixes=[\".py\"],\n",
    "            parser=LanguageParser(language=Language.PYTHON, parser_threshold=30),\n",
    "        )\n",
    "        documents.extend(loader.load())\n",
    "    print(f\".py 파일의 개수: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# 마크다운 파일 로드\n",
    "def load_markdown_files(repos):\n",
    "    documents = []\n",
    "    for repo in repos:\n",
    "        try:\n",
    "            loader = DirectoryLoader(\n",
    "                repo,\n",
    "                glob=\"**/*.md\",\n",
    "                loader_cls=TextLoader,\n",
    "                loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "                recursive=True,\n",
    "            )\n",
    "            documents.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading files from {repo}: {str(e)}\")\n",
    "            continue\n",
    "    print(f\".md 파일의 개수: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# 노트북 파일 로드\n",
    "def load_notebook_files(root_path):\n",
    "    notebook_files = glob.glob(root_path + \"/**/*.ipynb\", recursive=True)\n",
    "\n",
    "    # 노트북 파일을 마크다운 파일로 변환\n",
    "    notebook_md_files = []\n",
    "    for f in notebook_files:\n",
    "        converted_file = utils.convert_notebook_to_md(f)\n",
    "        notebook_md_files.append(converted_file)\n",
    "    notebook_md_files = list(set(notebook_md_files))\n",
    "\n",
    "    # 마크다운 파일 로드\n",
    "    documents = []\n",
    "    for f in notebook_md_files:\n",
    "        loader = TextLoader(f, encoding=\"utf-8\")\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = doc.metadata[\"source\"].replace(\".md\", \".ipynb\")\n",
    "        documents.extend(docs)\n",
    "        os.remove(f)\n",
    "\n",
    "    print(f\".ipynb 파일의 개수: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# 모든 문서 로드\n",
    "py_documents = load_python_files(all_repos)\n",
    "md_documents = load_markdown_files(all_repos)\n",
    "notebook_documents = load_notebook_files(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 문서의 개수를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = py_documents + md_documents + notebook_documents\n",
    "\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 분할\n",
    "\n",
    "Python 코드 파일은 문서 분할을 위해 RecursiveCharacterTextSplitter 를 사용합니다. 여기서 `Language.PYTHON` 은 파이썬 코드를 분할하는 데 사용됩니다.\n",
    "\n",
    "Text 형식의 파일은 RecursiveCharacterTextSplitter 에 별도의 옵션을 지정하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 분할을 위한 텍스트 스플리터 임포트\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Python 코드 전용 스플리터 생성\n",
    "# chunk_size: 각 청크의 최대 크기(문자 수)\n",
    "# chunk_overlap: 청크 간 중복되는 문자 수\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "\n",
    "# 일반 텍스트용 스플리터 생성\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "\n",
    "# 각 문서 타입별로 분할 수행\n",
    "split_py_documents = python_splitter.split_documents(py_documents)  # Python 파일 분할\n",
    "split_md_documents = text_splitter.split_documents(md_documents)  # Markdown 파일 분할\n",
    "split_notebook_documents = text_splitter.split_documents(\n",
    "    notebook_documents\n",
    ")  # Notebook 파일 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 문서를 합칩니다.\n",
    "split_docs = split_md_documents + split_md_documents + split_notebook_documents\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩 & 벡터 DB 에 저장\n",
    "\n",
    "다음으로는 분할한 문서를 임베딩하여 벡터 DB 에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import LocalFileStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 로컬 파일 저장소 설정\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "# 캐시를 지원하는 임베딩 생성\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings=embeddings,\n",
    "    document_embedding_cache=store,\n",
    "    namespace=embeddings.model,  # 기본 임베딩과 저장소를 사용하여 캐시 지원 임베딩을 생성\n",
    ")\n",
    "\n",
    "# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.\n",
    "db = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DB index 이름을 `LANGCHAIN_DB_INDEX` 로 설정하고 저장합니다.\n",
    "\n",
    "추후에 이 index 를 사용하여 저장된 벡터 DB 를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 DB 저장\n",
    "DB_INDEX = \"LANGCHAIN_DB_INDEX\"\n",
    "db.save_local(DB_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 저장된 DB 를 로드하는 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 DB 로드\n",
    "langgraph_db = FAISS.load_local(\n",
    "    DB_INDEX, embeddings, allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘 저장이 되었는지 테스트를 위하여 Query 를 입력하여 결과를 확인 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(langgraph_db.similarity_search(\"self-rag\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG 체인 구축\n",
    "\n",
    "여기서는 Naive 형태의 Chain 을 구성하도록 하겠습니다.\n",
    "\n",
    "단, Reranker 를 사용하여 검색 정확도를 향상시키도록 하겠습니다. (`Jina-reranker` 를 사용하지만, 다른 reranker 를 사용하거나, 빼도 좋습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import JinaRerank\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain import hub\n",
    "from operator import itemgetter\n",
    "\n",
    "# retriever 생성\n",
    "code_retriever = langgraph_db.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "# JinaRerank 설정\n",
    "compressor = JinaRerank(model=\"jina-reranker-v2-base-multilingual\", top_n=8)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=code_retriever\n",
    ")\n",
    "\n",
    "# prompt 설정\n",
    "system_prompt = \"\"\"You are an CODE Copilot Assistant. You must use the following pieces of retrieved source code or documentation to answer the question. \n",
    "You are given question related to RAG(Retrieval Augmented Generation) source code and documentation.\n",
    "If you don't know the answer, just say that you don't know. Answer in Korean.\n",
    "\n",
    "When answering questions, follow these guidelines:\n",
    "\n",
    "1. Use only the information provided in the context. \n",
    "2. Include as many example code snippets as possible.\n",
    "3. Writing a full code snippet is highly recommended.\n",
    "4. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "5. The context contain sources at the topic of each individual document.\n",
    "6. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
    "7. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
    "8. If the source is: <source>assistant/docs/llama3_1.md\" page=\"7\"</source>' then just list: \n",
    "        \n",
    "[1] llama3_1.md\n",
    "        \n",
    "And skip the addition of the brackets as well as the Document source preamble in your citation.\n",
    "\n",
    "----\n",
    "\n",
    "### Sources\n",
    "\n",
    "In the Sources section:\n",
    "- Include all sources used in your answer\n",
    "- Provide full links to relevant websites or document names\n",
    "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
    "- It will look like:\n",
    "\n",
    "**Sources**\n",
    "- [1] Link or Document name\n",
    "- [2] Link or Document name\n",
    "\n",
    "Be sure to combine sources. For example this is not correct:\n",
    "\n",
    "- [3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "- [4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "There should be no redundant sources. It should simply be:\n",
    "\n",
    "- [3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "-----\n",
    "\n",
    "### Retrieved Context\n",
    "\n",
    "Here is the context that you can use to answer the question:\n",
    "\n",
    "{context}\n",
    "\n",
    "----\n",
    "\n",
    "### Question\n",
    "\n",
    "Here is user's question:\n",
    "\n",
    "{question}\n",
    "\n",
    "----\n",
    "\n",
    "Final review:\n",
    "- Ensure the report follows the required structure\n",
    "- Check that all guidelines have been followed\n",
    "- Check if a full code snippet is included in your answer if applicable.\n",
    "- Your response should be written in Korean\n",
    "- Using many example code snippets would be rewarded by the user\n",
    "- Think step by step.\n",
    "\n",
    "----\n",
    "\n",
    "Your answer to the question with the source:\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm 설정\n",
    "llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"context\": itemgetter(\"context\"),\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph 체인 구축을 위한 노드 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing\n",
    "\n",
    "사용자의 질문에 대한 routing 을 수행합니다.\n",
    "\n",
    "질문이 vector 검색이 필요한지 여부를 판단하여 라우팅하며, 최대한 보수적으로 판단하도록 프롬프트를 설정하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "\n",
    "    # 데이터 소스 선택을 위한 리터럴 타입 필드\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question, determine if it needs to be retrieved from vectorstore or not. Return 'yes' if it needs to be retrieved from vectorstore, otherwise return 'no'.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM 초기화 및 함수 호출을 통한 구조화된 출력 생성\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# 시스템 메시지와 사용자 질문을 포함한 프롬프트 템플릿 생성\n",
    "system = \"\"\"You are an expert at routing a user question. \n",
    "The vectorstore contains documents related to RAG(Retrieval Augmented Generation) source code and documentation.\n",
    "Return 'yes' if the question is related to the source code or documentation, otherwise return 'no'.\n",
    "If you can't determine if the question is related to the source code or documentation, return 'yes'.\n",
    "If you don't know the answer, return 'yes'.\"\"\"\n",
    "\n",
    "# Routing 을 위한 프롬프트 템플릿 생성\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 프롬프트 템플릿과 구조화된 LLM 라우터를 결합하여 질문 라우터 생성\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇 개의 Query 를 입력하여 결과를 확인 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_router.invoke({\"question\": \"Self-RAG 에 대해 설명하세요.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_router.invoke({\"question\": \"MemorySaver 에 대해 설명하세요.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_router.invoke({\"question\": \"대한민국의 수도는?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Rewrite\n",
    "\n",
    "다음으로는 사용자의 질문에 대한 질문을 재작성(query rewrite)하는 노드를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM 설정\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# Query Rewrite 시스템 프롬프트\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized for CODE SEARCH(github repository). \n",
    "Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "\n",
    "Base Code Repository: \n",
    "\n",
    "https://github.com/langchain-ai/langgraph\n",
    "\n",
    "Output should be in English.\"\"\"\n",
    "\n",
    "# 프롬프트 정의\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Question Re-writer 체인 초기화\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 확인 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_rewriter.invoke({\"question\": \"Self-RAG 에 대해 설명하세요.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가\n",
    "\n",
    "중간 단계에 문서의 관련성 평가와 답변의 환각 여부 평가를 수행합니다. 따라서 다음의 3개 노드가 추가로 필요합니다.\n",
    "\n",
    "- 검색된 문서의 관련성 평가\n",
    "- 답변의 환각 여부 평가\n",
    "- 답변의 질문에 대한 관련성 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색된 문서의 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# 문서 평가를 위한 데이터 모델 정의\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM 초기화 및 함수 호출을 통한 구조화된 출력 생성\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# 시스템 메시지와 사용자 질문을 포함한 프롬프트 템플릿 생성\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 문서 검색결과 평가기 생성\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 답변의 환각 여부 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 할루시네이션 체크를 위한 데이터 모델 정의\n",
    "class AnswerGroundedness(BaseModel):\n",
    "    \"\"\"Binary score for answer groundedness.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts(given context), 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM 설정\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(AnswerGroundedness)\n",
    "\n",
    "# 프롬프트 설정\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "groundedness_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 답변의 환각 여부 평가기 생성\n",
    "groundedness_checker = groundedness_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 답변의 질문에 대한 관련성 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary scoring to evaluate the appropriateness of answers to questions\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicate 'yes' or 'no' whether the answer solves the question\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 함수 호출을 통한 LLM 초기화\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# 프롬프트 설정\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "relevant_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 프롬프트 템플릿과 구조화된 LLM 평가기를 결합하여 답변 평가기 생성\n",
    "relevant_answer_checker = relevant_answer_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 도구\n",
    "\n",
    "여기서는 TavilySearch 를 사용하여 웹 검색을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# 웹 검색 도구 생성\n",
    "web_search_tool = TavilySearch(max_results=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상태 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 그래프의 상태 정의\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    그래프의 상태를 나타내는 데이터 모델\n",
    "\n",
    "    Attributes:\n",
    "        question: 질문\n",
    "        generation: LLM 생성된 답변\n",
    "        documents: 도큐먼트 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    question: Annotated[str, \"User question\"]\n",
    "    generation: Annotated[str, \"LLM generated answer\"]\n",
    "    documents: Annotated[List[Document], \"List of documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 노드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# 질문 라우팅 노드\n",
    "def route_question_node(state):\n",
    "    question = state[\"question\"]\n",
    "    evaluation = question_router.invoke({\"question\": question})\n",
    "\n",
    "    # 질문 라우팅 결과에 따른 노드 라우팅\n",
    "    if evaluation.binary_score == \"yes\":\n",
    "        return \"query_expansion\"\n",
    "    else:\n",
    "        return \"general_answer\"\n",
    "\n",
    "\n",
    "# 질문 재작성 노드\n",
    "def query_rewrite_node(state):\n",
    "    # 질문과 문서 검색 결과 가져오기\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 질문 재작성\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"question\": better_question}\n",
    "\n",
    "\n",
    "# 문서 검색 노드\n",
    "def retrieve_node(state):\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 문서 검색 수행\n",
    "    documents = compression_retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "# 답변 생성 노드\n",
    "def general_answer_node(state):\n",
    "    # 질문 가져오기\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # RAG 답변 생성\n",
    "    answer = llm.invoke(question)\n",
    "    return {\"generation\": answer}\n",
    "\n",
    "\n",
    "# RAG 답변 생성 노드\n",
    "def rag_answer_node(state):\n",
    "    # 질문과 문서 검색 결과 가져오기\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG 답변 생성\n",
    "    answer = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"generation\": answer}\n",
    "\n",
    "\n",
    "# 문서의 관련성을 평가 후 필터링\n",
    "def filtering_documents_node(state):\n",
    "    # 질문과 문서 검색 결과 가져오기\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 각 문서에 대한 관련성 점수 계산\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            # 관련성이 있는 문서 추가\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            # 관련성이 없는 문서는 건너뛰기\n",
    "            continue\n",
    "\n",
    "    return {\"documents\": filtered_docs}\n",
    "\n",
    "\n",
    "# 웹 검색 노드: Document\n",
    "def web_search_node(state):\n",
    "    # 질문과 문서 검색 결과 가져오기\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 웹 검색 수행\n",
    "    web_results = web_search_tool.invoke({\"query\": question})\n",
    "    web_results_docs = [\n",
    "        Document(\n",
    "            page_content=web_result[\"content\"],\n",
    "            metadata={\"source\": web_result[\"url\"]},\n",
    "        )\n",
    "        for web_result in web_results\n",
    "    ]\n",
    "\n",
    "    return {\"documents\": web_results_docs}\n",
    "\n",
    "\n",
    "# 추가 정보 검색 필요성 여부 평가 노드\n",
    "def decide_to_web_search_node(state):\n",
    "    # 문서 검색 결과 가져오기\n",
    "    filtered_docs = state[\"documents\"]\n",
    "\n",
    "    if len(filtered_docs) < 2:\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        return \"rag_answer\"\n",
    "\n",
    "\n",
    "# 답변의 환각 여부/관련성 여부 평가 노드\n",
    "def answer_groundedness_check(state):\n",
    "    # 질문과 문서 검색 결과 가져오기\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    # Groundedness 평가\n",
    "    score = groundedness_checker.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Groundedness 평가 결과에 따른 처리\n",
    "    if grade == \"yes\":\n",
    "        # 답변의 관련성(Relevance) 평가\n",
    "        score = relevant_answer_checker.invoke(\n",
    "            {\"question\": question, \"generation\": generation}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "\n",
    "        # 관련성 평가 결과에 따른 처리\n",
    "        if grade == \"yes\":\n",
    "            return \"relevant\"\n",
    "        else:\n",
    "            return \"not relevant\"\n",
    "\n",
    "    else:\n",
    "        return \"not grounded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 그래프 상태 초기화\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"query_expand\", query_rewrite_node)  # 질문 재작성\n",
    "workflow.add_node(\"query_rewrite\", query_rewrite_node)  # 질문 재작성\n",
    "workflow.add_node(\"web_search\", web_search_node)  # 웹 검색\n",
    "workflow.add_node(\"retrieve\", retrieve_node)  # 문서 검색\n",
    "workflow.add_node(\"grade_documents\", filtering_documents_node)  # 문서 평가\n",
    "workflow.add_node(\"general_answer\", general_answer_node)  # 일반 답변 생성\n",
    "workflow.add_node(\"rag_answer\", rag_answer_node)  # RAG 답변 생성\n",
    "\n",
    "# 엣지 추가\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question_node,\n",
    "    {\n",
    "        \"query_expansion\": \"query_expand\",  # 웹 검색으로 라우팅\n",
    "        \"general_answer\": \"general_answer\",  # 벡터스토어로 라우팅\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"query_expand\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_web_search_node,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",  # 웹 검색 필요\n",
    "        \"rag_answer\": \"rag_answer\",  # RAG 답변 생성 가능\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"query_rewrite\", \"rag_answer\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"rag_answer\",\n",
    "    answer_groundedness_check,\n",
    "    {\n",
    "        \"relevant\": END,\n",
    "        \"not relevant\": \"web_search\",\n",
    "        \"not grounded\": \"query_rewrite\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"web_search\", \"rag_answer\")\n",
    "\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, random_uuid\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\n",
    "    \"question\": \"Self-RAG 에서 사용되는 관련성 평가 노드 예제를 찾아줘\",\n",
    "}\n",
    "\n",
    "# 스트리밍 형식으로 그래프 실행\n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs,\n",
    "    config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seoulrnd-graphrag-gR8UbrDn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
