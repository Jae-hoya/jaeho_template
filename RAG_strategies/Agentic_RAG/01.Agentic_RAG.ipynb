{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317b3ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6e746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retriever import QdrantRetrieverFactory\n",
    "\n",
    "qs = QdrantRetrieverFactory()\n",
    "\n",
    "retriever = qs.retriever(collection_name=\"RAG_Example(RAG_strategies)\", fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41aa7b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'docling_outputs\\\\SPRi AI Brief_8ì›”í˜¸_ì‚°ì—…ë™í–¥_F.md', '_id': '99fab946-a7a5-44fc-a827-38cba5e9b3ca', '_collection_name': 'RAG_Example(RAG_strategies)'}, page_content=\"- n AI ì´ë¯¸ì§€ ìƒì„± í”Œë«í¼ ë¯¸ë“œì €ë‹ˆ(Midjourney)ê°€ 2025ë…„ 6ì›” 19ì¼ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ 'V1'ì„ ì¶œì‹œ\\n- V1ì€ ì´ë¯¸ì§€ë¥¼ ë™ì˜ìƒìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸ë¡œ, ë¯¸ë“œì €ë‹ˆ í”Œë«í¼ì—ì„œ ì œì‘ëœ ì´ë¯¸ì§€ë‚˜ ì™¸ë¶€ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë™ì˜ìƒì„ ìƒì„±í•˜ë©°, 'ìë™' ì„¤ì • ì‹œì—ëŠ” ëª¨ì…˜ í”„ë¡¬í”„íŠ¸ê°€ ìë™ìœ¼ë¡œ ìƒì„±ë˜ê³  'ìˆ˜ë™' ì„¤ì •ì„ ì„ íƒí•˜ë©´ ì‚¬ìš©ì ì§€ì‹œì— ë”°ë¼ ì‚¬ë¬¼ì´ë‚˜ ì¥ë©´ì˜ ì›€ì§ì„ì„ ìƒì„±\\n- ì‚¬ìš©ìëŠ” ì›€ì§ì„ ê°•ë„ë¥¼ í”¼ì‚¬ì²´ì™€ ì¹´ë©”ë¼ê°€ ëª¨ë‘ ì›€ì§ì´ëŠ” 'í•˜ì´ ëª¨ì…˜(High Motion)'ê³¼ ì¹´ë©”ë¼ëŠ” ê±°ì˜ ê³ ì •ë˜ì–´ ìˆê³  í”¼ì‚¬ì²´ê°€ ì²œì²œíˆ ì›€ì§ì´ë„ë¡ ì—°ì¶œ 'ë¡œìš° ëª¨ì…˜(Low Motion)' ì¤‘ì—ì„œ ì„ íƒ ê°€ëŠ¥\\n- ì›¹ ì „ìš© ëª¨ë¸ì¸ V1ì€ 1íšŒ ë™ì˜ìƒ ìƒì„± ì‘ì—…ìœ¼ë¡œ 5ì´ˆ ê¸¸ì´ì˜ ë™ì˜ìƒ 4ê°œë¥¼ ì œì‘í•˜ë©°, ìƒì„±ëœ ë™ì˜ìƒì€ í•œ ë²ˆì— 4ì´ˆì”© ìµœëŒ€ 4íšŒê¹Œì§€ ì˜ìƒ ê¸¸ì´ë¥¼ í™•ì¥í•  ìˆ˜ ìˆë„ë¡ í—ˆìš©\"),\n",
       " Document(metadata={'source': 'docling_outputs\\\\SPRi AI Brief_8ì›”í˜¸_ì‚°ì—…ë™í–¥_F.md', '_id': '3b53a337-de9e-47b0-a2f4-eaedb4519aee', '_collection_name': 'RAG_Example(RAG_strategies)'}, page_content=\"<!-- image -->\\n\\n## ê¸°ì—… ï½¥ ì‚°ì—…\\n\\n<!-- image -->\\n\\n## ë¯¸ë“œì €ë‹ˆ, ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ ìƒì„± AI ëª¨ë¸ 'V1' ì¶œì‹œ\\n\\n## KEY Contents\\n\\n- n ë¯¸ë“œì €ë‹ˆê°€ 1íšŒ ì‘ì—…ìœ¼ë¡œ 5ì´ˆ ê¸¸ì´ì˜ ë™ì˜ìƒ 4ê°œë¥¼ ì œì‘í•  ìˆ˜ ìˆëŠ” ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ 'V1'ì„ ì¶œì‹œí•˜ê³  ì—¬íƒ€ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ë³´ë‹¤ 25ë°° ì´ìƒ ì €ë ´í•œ ê°€ê²©ì„ ì±…ì •í–ˆë‹¤ê³  ê°•ì¡°\\n- n ë¯¸ë“œì €ë‹ˆëŠ” ì‹¤ì‹œê°„ ì˜¤í”ˆì›”ë“œ ì‹œë®¬ë ˆì´ì…˜ì´ ê°€ëŠ¥í•œ AI ëª¨ë¸ ê°œë°œì„ ê¶ê·¹ì  ëª©í‘œë¡œ ì œì‹œí•˜ê³ , 2026ë…„ì— 3D ëª¨ë¸ê³¼ ì‹¤ì‹œê°„ ì²˜ë¦¬ ëª¨ë¸ì„ ì¶œì‹œí•œ ë’¤ ê° ëª¨ë¸ì„ í•˜ë‚˜ë¡œ í†µí•©í•  ê³„íšì´ë¼ê³  ì„¤ëª…\\n\\n## Â£ 'V1', ë¯¸ë“œì €ë‹ˆì—ì„œ ìƒì„±í•œ ì´ë¯¸ì§€ë‚˜ ì™¸ë¶€ ì´ë¯¸ì§€ë¥¼ ë™ì˜ìƒìœ¼ë¡œ ë³€í™˜\"),\n",
       " Document(metadata={'source': 'docling_outputs\\\\SPRi AI Brief_8ì›”í˜¸_ì‚°ì—…ë™í–¥_F.md', '_id': '1517467d-f582-48df-a032-a9cff3d84d35', '_collection_name': 'RAG_Example(RAG_strategies)'}, page_content=\"|               | âˆ™ EU ì§‘í–‰ìœ„ì›íšŒ, ã€ŒAI ë²•ã€ì˜ ë²”ìš© AI ê´€ë ¨ ì‹¤ì²œ ê°•ë ¹ê³¼ ì§€ì¹¨ ë°œí‘œ       |   6 |\\n| ê¸°ì—… ï½¥ ì‚°ì—…   | âˆ™ ë¯¸ë“œì €ë‹ˆ, ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ ìƒì„± AI ëª¨ë¸ 'V1' ì¶œì‹œ                     |   8 |\\n|               | âˆ™ ë°”ì´ë‘,10ë…„ë§Œì—AIê¸°ë°˜ìœ¼ë¡œê²€ìƒ‰ì„œë¹„ìŠ¤ëŒ€í­ê°œí¸                         |   9 |\\n|               | âˆ™ ë¬¸ìƒ· AI, ì—ì´ì „íŠ¸ ê¸°ëŠ¥ ì§€ì›í•˜ëŠ” 'í‚¤ë¯¸ K2'ì˜¤í”ˆì†ŒìŠ¤ ê³µê°œ              |  10 |\\n|               | âˆ™ xAI, ì°¨ì„¸ëŒ€ AI ëª¨ë¸ 'ê·¸ë¡ 4'ê³µê°œ ë° ì •ë¶€ AI ì‹œì¥ ì§„ì¶œ               |  11 |\\n|               | âˆ™ í¼í”Œë ‰ì‹œí‹°, AI ì—ì´ì „íŠ¸ íƒ‘ì¬í•œ ì›¹ ë¸Œë¼ìš°ì € 'ì½”ë©§'ì¶œì‹œ               |  12 |\")]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"ë¯¸ë“œì €ë‹ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79bf9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag import rag\n",
    "\n",
    "rag_chain = rag(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e628cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ë¯¸ë“œì €ë‹ˆ(Midjourney)ëŠ” AI ì´ë¯¸ì§€ ìƒì„± í”Œë«í¼ìœ¼ë¡œ, 2025ë…„ 6ì›” 19ì¼ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ 'V1'ì„ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤. V1ì€ ë¯¸ë“œì €ë‹ˆì—ì„œ ìƒì„±í•œ ì´ë¯¸ì§€ë‚˜ ì™¸ë¶€ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ 5ì´ˆ ê¸¸ì´ì˜ ë™ì˜ìƒ 4ê°œë¥¼ í•œ ë²ˆì— ì œì‘í•  ìˆ˜ ìˆìœ¼ë©°, ë™ì˜ìƒ ê¸¸ì´ëŠ” ìµœëŒ€ 4ì´ˆì”© 4íšŒê¹Œì§€ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” 'í•˜ì´ ëª¨ì…˜(High Motion)'ê³¼ 'ë¡œìš° ëª¨ì…˜(Low Motion)' ì¤‘ ì›€ì§ì„ ê°•ë„ë¥¼ ì„ íƒí•  ìˆ˜ ìˆê³ , ìë™ ë˜ëŠ” ìˆ˜ë™ ëª¨ì…˜ í”„ë¡¬í”„íŠ¸ ì„¤ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë¯¸ë“œì €ë‹ˆëŠ” 2026ë…„ì— 3D ëª¨ë¸ê³¼ ì‹¤ì‹œê°„ ì²˜ë¦¬ ëª¨ë¸ì„ ì¶œì‹œí•´ í†µí•©í•  ê³„íšì´ë©°, V1ì€ ì—¬íƒ€ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ë³´ë‹¤ 25ë°° ì´ìƒ ì €ë ´í•œ ê°€ê²©ì„ ì±…ì •í–ˆìŠµë‹ˆë‹¤.\\n\\n**Source**  \\n- docling_outputs\\\\SPRi AI Brief_8ì›”í˜¸_ì‚°ì—…ë™í–¥_F.md (p.8)\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({\"question\": \"ë¯¸ë“œì €ë‹ˆ\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d70dbd",
   "metadata": {},
   "source": [
    "# ìƒíƒœ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8519f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence, Optional\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import add_messages\n",
    "\n",
    "# create_react_agentì—ì„œëŠ” messagesë¥¼ í‚¤ë¡œ ì‚¬ìš©í•œë‹¤.\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    next: Optional[str]  # ë‹¤ìŒì— ì‹¤í–‰í•  ë…¸ë“œë¥¼ ì§€ì •í•˜ëŠ” í•„ë“œ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136753eb",
   "metadata": {},
   "source": [
    "# create_retriever_tool\n",
    "\n",
    "retrieverë¥¼ í•˜ë‚˜ì˜ ë„êµ¬ë¡œ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc5e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "retriever_tool = create_retriever_tool(retriever=retriever, name=\"retriever\",\n",
    "                                       description=\"Search and return information about SPRI AI Brief PDF file. It contains useful information on recent AI trends. The document is published on Dec 2025\",\n",
    "                                       document_prompt=PromptTemplate.from_template(\n",
    "                                           \"<document><content>{page_content}</content><metadata><source>{source}</source><page></page></metadata></document>\"\n",
    "                                            )\n",
    "                                       )\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cc0a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"ì—ì´ì „í‹± RAGì˜ ì¥ì ì€?\"\n",
    "# result = retriever_tool.invoke(query)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9f7cb",
   "metadata": {},
   "source": [
    "# create_react_agent: Agentìƒì„±\n",
    "\n",
    "create_react_agentëŠ”, messagesë¡œ ëŒ€í™”ë¥¼ ì£¼ê³  ë°›ëŠ”ê²Œ íŠ¹ì§•ì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac71483",
   "metadata": {},
   "source": [
    "### retriever_agent ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7dfb442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base import make_system_prompt\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "retriever_agent = create_react_agent(llm, tools=tools,\n",
    "                                    state_modifier=make_system_prompt(\n",
    "                                    \"\"\"You are a specialized research agent focused on retrieving and analyzing information from SPRI AI Brief documents.\n",
    "                                    \n",
    "                                    Your primary responsibilities:\n",
    "                                    1. Search through SPRI AI Brief PDF documents using the retriever tool\n",
    "                                    2. Find relevant, authoritative information about AI trends, technologies, and industry developments\n",
    "                                    3. Provide detailed, well-sourced answers based on the retrieved documents\n",
    "                                    4. Always cite your sources and provide context for the information you share\n",
    "                                    \n",
    "                                    You work collaboratively with other agents:\n",
    "                                    - Researcher agent: Handles web-based research and current information\n",
    "                                    - Coder agent: Creates visualizations and charts based on your findings\n",
    "                                    \n",
    "                                    When retrieving information:\n",
    "                                    - Use specific, targeted search queries\n",
    "                                    - Look for the most relevant and recent information\n",
    "                                    - Provide comprehensive answers with proper context\n",
    "                                    - Always mention the source of your information\n",
    "                                    \n",
    "                                    Remember: You can ONLY use the retriever tool to search SPRI documents. For other types of research, coordinate with the Researcher agent.\"\"\"), \n",
    "                                    checkpointer=memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdac002",
   "metadata": {},
   "source": [
    "# retriever_node ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f2da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "def retriever_node(state: AgentState) -> AgentState:\n",
    "    # invoke ì‹œ ë¶„ë¦¬\n",
    "    result = retriever_agent.invoke(\n",
    "        {\"messages\": state[\"messages\"]}, \n",
    "    )\n",
    "\n",
    "    last_ai = [m for m in result[\"messages\"] if m.type == \"ai\"][-1]\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=last_ai.content, name=\"retriever\")],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7e941",
   "metadata": {},
   "source": [
    "### Research Agent ìƒì„±\n",
    "\n",
    "TavilySearch ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6105a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "tavily_tool = TavilySearch(max_results=5)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "research_agent = create_react_agent(llm, tools=[tavily_tool],\n",
    "                                    state_modifier=make_system_prompt(\n",
    "                                        \"You can only do research. You are working with a chart generator colleague.\"\n",
    "                                    ),\n",
    "                                    checkpointer=MemorySaver()\n",
    "                                    )\n",
    "def research_node(state: MessagesState) -> MessagesState:\n",
    "    result = research_agent.invoke(\n",
    "        {\"messages\": state[\"messages\"]},\n",
    "    )\n",
    "    last_message = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"research\"\n",
    "    )\n",
    "    return {\n",
    "        \"messages\": [last_message],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0ea2a",
   "metadata": {},
   "source": [
    "# Chart Generator Agent\n",
    "PythonREPL ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ì—ì´ì „íŠ¸ë¥¼ ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "íŒŒì´ì¬ repl ë„êµ¬ë¥¼ ìƒˆë¡œ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cffcd5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "python_repl = PythonREPLTool()\n",
    "\n",
    "# Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë„êµ¬ ì •ì˜\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        # ì£¼ì–´ì§„ ì½”ë“œë¥¼ Python REPLì—ì„œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ë°˜í™˜\n",
    "        result = python_repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute code. Error: {repr(e)}\"\n",
    "    # ì‹¤í–‰ ì„±ê³µ ì‹œ ê²°ê³¼ì™€ í•¨ê»˜ ì„±ê³µ ë©”ì‹œì§€ ë°˜í™˜\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "861d66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_system_prompt = \"\"\"\n",
    "Be sure to use the following font in your code for visualization.\n",
    "\n",
    "##### í°íŠ¸ ì„¤ì • #####\n",
    "import platform\n",
    "\n",
    "# OS íŒë‹¨\n",
    "current_os = platform.system()\n",
    "\n",
    "if current_os == \"Windows\":\n",
    "    # Windows í™˜ê²½ í°íŠ¸ ì„¤ì •\n",
    "    font_path = \"C:/Windows/Fonts/malgun.ttf\"  # ë§‘ì€ ê³ ë”• í°íŠ¸ ê²½ë¡œ\n",
    "    fontprop = fm.FontProperties(fname=font_path, size=12)\n",
    "    plt.rc(\"font\", family=fontprop.get_name())\n",
    "elif current_os == \"Darwin\":  # macOS\n",
    "    # Mac í™˜ê²½ í°íŠ¸ ì„¤ì •\n",
    "    plt.rcParams[\"font.family\"] = \"AppleGothic\"\n",
    "else:  # Linux ë“± ê¸°íƒ€ OS\n",
    "    # ê¸°ë³¸ í•œê¸€ í°íŠ¸ ì„¤ì • ì‹œë„\n",
    "    try:\n",
    "        plt.rcParams[\"font.family\"] = \"NanumGothic\"\n",
    "    except:\n",
    "        print(\"í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "##### ë§ˆì´ë„ˆìŠ¤ í°íŠ¸ ê¹¨ì§ ë°©ì§€ #####\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # ë§ˆì´ë„ˆìŠ¤ í°íŠ¸ ê¹¨ì§ ë°©ì§€\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Chart Generator Agent ìƒì„±\n",
    "coder_agent = create_react_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    state_modifier=code_system_prompt,\n",
    "    checkpointer=MemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f783045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coder_node(state: MessagesState) -> MessagesState:\n",
    "    result = coder_agent.invoke(state)\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ HumanMessage ë¡œ ë³€í™˜\n",
    "    last_message = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"coder\",\n",
    "        # config = state[\"config\"]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        # share internal message history of chart agent with other agents\n",
    "        \"messages\": [last_message],\n",
    "        # \"config\": state[\"config\"]    \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7164afb4",
   "metadata": {},
   "source": [
    "# ì¼ë°˜ì§ˆë¬¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88fdb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Agent ìƒì„± (react_agentë¡œ ë³€ê²½)\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# General Agentë¥¼ react_agentë¡œ ìƒì„±\n",
    "general_agent = create_react_agent(\n",
    "    llm, \n",
    "    tools=[],  # ë„êµ¬ ì—†ìŒ (ì¼ë°˜ì ì¸ ëŒ€í™”ë§Œ)\n",
    "    state_modifier=make_system_prompt(\n",
    "        \"You are a helpful general assistant. You can answer general questions, \"\n",
    "        \"have conversations, and provide information. You work with other specialized agents: \"\n",
    "        \"- Retriever agent: For searching SPRI documents \"\n",
    "        \"- Researcher agent: For web research \"\n",
    "        \"- Coder agent: For creating charts and visualizations \"\n",
    "        \"If you need specialized information, coordinate with the appropriate agent.\"\n",
    "    ),\n",
    "    checkpointer=MemorySaver()\n",
    ")\n",
    "\n",
    "def general_node(state: MessagesState) -> MessagesState:\n",
    "    # react_agent ì‚¬ìš©\n",
    "    result = general_agent.invoke(\n",
    "        {\"messages\": state[\"messages\"]},\n",
    "    )\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ HumanMessageë¡œ ë³€í™˜\n",
    "    last_message = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"general\",\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [last_message],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ab5c9",
   "metadata": {},
   "source": [
    "# Agent Supervisor ìƒì„±\n",
    "ì—ì´ì „íŠ¸ë¥¼ ê´€ë¦¬ ê°ë…í•˜ëŠ” ê°ë…ì ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a2eb417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "members = [\"Retriever\", \"Researcher\", \"Coder\", \"General LLM\"]\n",
    "\n",
    "options_for_next = [\"FINISH\"] + members\n",
    "\n",
    "class RouteResponse(BaseModel):\n",
    "    next: Literal[*options_for_next]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb172e",
   "metadata": {},
   "source": [
    "partialì€ olptionsì™€ membersë¥¼ ì±„ìš°ëŠ” ê°’ì¸ë°, ì–´ë–»ê²Œ ì±„ì›Œì•¼ í•˜ëŠ”ì§€ ê³ ì •ê°’ì´ ì •í•´ì§€ëŠ”ê²ƒì´ë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ, `invoke`í• ë•Œ ë”°ë¡œ ë„£ì–´ì£¼ì§€ ì•Šì•„ë„ ë˜ëŠ” ê°’ì´ë‹¤.\n",
    "\n",
    "ì´ì¤‘ì—ì„œ `options_for_next` ì¤‘ì— í•˜ë‚˜ë¥¼ ë½‘ê²Œ ë˜ëŠ” prompt, chainì´ ë  ê²ƒì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78a01270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜: ì‘ì—…ì ê°„ì˜ ëŒ€í™”ë¥¼ ê´€ë¦¬í•˜ëŠ” ê°ë…ì ì—­í• \n",
    "# system_prompt = (\n",
    "#     \"You are a supervisor tasked with managing a conversation between the\"\n",
    "#     \" following workers:  {members}. Given the following user request,\"\n",
    "#     \" respond with the worker to act next. Each worker will perform a\"\n",
    "#     \" task and respond with their results and status.\"\n",
    "    \n",
    "#     \" CRITICAL RULE: If a question comes up, you MUST answer it. Don't just send it to FINISH!\"\n",
    "    \n",
    "#     \" IMPORTANT FINISH CONDITIONS:\"\n",
    "#     \" - FINISH only when the user's question has been fully answered\"\n",
    "#     \" - FINISH only when at least one worker has provided a complete response\"\n",
    "#     \" - FINISH only when no additional information or action is needed\"\n",
    "#     \" - Do NOT FINISH if the response is incomplete or needs clarification\"\n",
    "#     \" - Do NOT FINISH if the user's question is not answered\"\n",
    "#     \" WORKER SELECTION RULES:\"\n",
    "#     \" - You must select at least one worker before considering FINISH\"\n",
    "#     \" - Do not select FINISH immediately - always try a worker first\"\n",
    "#     \" - Do not select the same worker more than 2 times in a row\"\n",
    "#     \" - If a worker's response is incomplete, select another appropriate worker\"\n",
    "# )\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         MessagesPlaceholder(variable_name=\"messages\"),\n",
    "#         (\n",
    "#             \"system\",\n",
    "#             \"Given the conversation above, who should act next?\"\n",
    "#             \"Or should we FINISH? Select one of: {options}\"\n",
    "            \n",
    "#             \" CRITICAL: You MUST answer the user's question before selecting FINISH!\"\n",
    "            \n",
    "#             \" DECISION CRITERIA:\"\n",
    "#             \" - If the user's question is fully answered with complete information, select FINISH\"\n",
    "#             \" - If the response is incomplete or needs more information, select an appropriate worker\"\n",
    "#             \" - If a worker has provided a complete response, consider FINISH\"\n",
    "#             \" - If a worker's response is insufficient, select another worker\"\n",
    "#             \" - NEVER select FINISH if the user's question is not answered\"\n",
    "            \n",
    "#             \" IMPORTANT RULES:\"\n",
    "#             \" - Do not select the same {members} more than 2 times in a row\"\n",
    "#             \" - You must select at least one {members} before considering FINISH\"\n",
    "#             \" - Only select FINISH when the task is truly complete\"\n",
    "#         )\n",
    "#     ]\n",
    "# ).partial(options=str(options_for_next), members=\", \".join(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "742eb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜: ì‘ì—…ì ê°„ì˜ ëŒ€í™”ë¥¼ ê´€ë¦¬í•˜ëŠ” ê°ë…ì ì—­í• \n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    "    \" If a worker has already provided a response, consider if FINISH is appropriate.\"\n",
    "    \" task and respond with their results and status.\"\n",
    "    \" You must select at least one worker before considering FINISH.\"\n",
    "    \" Do not select FINISH immediately - always try a worker first.\"\n",
    "    \" Do not select the same worker({members}) more than once times in a row.\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \"Or should we FINISH? Select one of: {options}\"\n",
    "            \" IMPORTANT: Do not select the same worker({members}) more than once times in a row.\"\n",
    "            \" IMPORTANT: You must select at least one worker({members}) before considering FINISH.\"\n",
    "            \" If a worker has already provided a response, consider FINISH.\"\n",
    "        )\n",
    "    ]\n",
    ").partial(options=str(options_for_next), members=\", \".join(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a92413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_agent(state):\n",
    "    llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "    supervisor_chain = prompt | llm.with_structured_output(RouteResponse)\n",
    "    return supervisor_chain.invoke(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7283c9",
   "metadata": {},
   "source": [
    "# ê·¸ë˜í”„ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f01c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# ê·¸ë˜í”„ì— ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"Retriever\", retriever_node)\n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"Coder\", coder_node)\n",
    "workflow.add_node(\"General LLM\", general_node)\n",
    "\n",
    "workflow.add_node(\"Supervisor\", supervisor_agent)\n",
    "\n",
    "\n",
    "# ë©¤ë²„ ë…¸ë“œ > Supervisor ë…¸ë“œë¡œ ì—£ì§€ ì¶”ê°€\n",
    "for member in members:\n",
    "    workflow.add_edge(member, \"Supervisor\")\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€ (\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "\n",
    "\n",
    "def get_next(state):\n",
    "    return state[\"next\"]\n",
    "\n",
    "\n",
    "# Supervisor ë…¸ë“œì—ì„œ ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_conditional_edges(\"Supervisor\", get_next, conditional_map)\n",
    "\n",
    "# ì‹œì‘ì \n",
    "workflow.add_edge(START, \"Supervisor\")\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d3ed317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"next\":\"General LLM\"}\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì•ˆë…•í•˜ì„¸ìš”! ë‹¤ì‹œ ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mGeneral LLM\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì•ˆë…•í•˜ì„¸ìš”! ë‹¤ì‹œ ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"next\":\"FINISH\"}"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph, stream_graph\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": 123})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"ì•ˆë…•?\"\n",
    "        ),\n",
    "        \n",
    "    ],\"config\": config\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(graph, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99fe9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.runnables import RunnableConfig\n",
    "# from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "# # config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "# config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# # ì§ˆë¬¸ ì…ë ¥\n",
    "# inputs = {\n",
    "#     \"messages\": [\n",
    "#         HumanMessage(\n",
    "#             content=\"2010ë…„ ~ 2024ë…„ê¹Œì§€ì˜ ëŒ€í•œë¯¼êµ­ì˜ 1ì¸ë‹¹ GDP ì¶”ì´ë¥¼ ê·¸ë˜í”„ë¡œ ì‹œê°í™” í•´ì£¼ì„¸ìš”.\"\n",
    "#         ),\n",
    "        \n",
    "#     ],\"config\": config\n",
    "# }\n",
    "\n",
    "# # ê·¸ë˜í”„ ì‹¤í–‰\n",
    "# invoke_graph(graph, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25951a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mnext\u001b[0m:\n",
      "Retriever\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mRetriever\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retriever (call_RTupIlWikN0XlkoU4YHSZRD8)\n",
      " Call ID: call_RTupIlWikN0XlkoU4YHSZRD8\n",
      "  Args:\n",
      "    query: Midjourney new version\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m in [\u001b[1;33mRetriever\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever\n",
      "\n",
      "<document><content>| 12ì›” | 3~5ì¼   | (ì†Œí”„íŠ¸ì›¨ì´ë¸Œ 2025)10íšŒëŒ€í•œë¯¼êµ­ì†Œí”„íŠ¸ì›¨ì–´ëŒ€ì „                                     | ì„œìš¸, ê°•ë‚¨                | www.k-softwave.com                                       |\n",
      "| 12ì›” | 10~11ì¼ | AI Summit New York                                                                | ë¯¸êµ­, ë‰´ìš•                | newyork.theaisummit.com                                  |</content><metadata><source>docling_outputs\\SPRi AI Brief_8ì›”í˜¸_ì‚°ì—…ë™í–¥_F.md</source><page></page></metadata></document>\n",
      "\n",
      "<document><content>- n AI ì´ë¯¸ì§€ ìƒì„± í”Œë«í¼ ë¯¸ë“œì €ë‹ˆ(Midjourney)ê°€ 2025ë…„ 6ì›” 19ì¼ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ 'V1'ì„ ì¶œì‹œ\n",
      "- V1ì€ ì´ë¯¸ì§€ë¥¼ ë™ì˜ìƒìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸ë¡œ, ë¯¸ë“œì €ë‹ˆ í”Œë«í¼ì—ì„œ ì œì‘ëœ ì´ë¯¸ì§€ë‚˜ ì™¸ë¶€ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë™ì˜ìƒì„ ìƒì„±í•˜ë©°, 'ìë™' ì„¤ì • ì‹œì—ëŠ” ëª¨ì…˜ í”„ë¡¬í”„íŠ¸ê°€ ìë™ìœ¼ë¡œ ìƒì„±ë˜ê³  'ìˆ˜ë™' ì„¤ì •ì„ ì„ íƒí•˜ë©´ ì‚¬ìš©ì ì§€ì‹œì— ë”°ë¼ ì‚¬ë¬¼ì´ë‚˜ ì¥ë©´ì˜ ì›€ì§ì„ì„ ìƒì„±\n",
      "- ì‚¬ìš©ìëŠ” ì›€ì§ì„ ê°•ë„ë¥¼ í”¼ì‚¬ì²´ì™€ ì¹´ë©”ë¼ê°€ ëª¨ë‘ ì›€ì§ì´ëŠ” 'í•˜ì´ ëª¨ì…˜(High Motion)'ê³¼ ì¹´ë©”ë¼ëŠ” ê±°ì˜ ê³ ì •ë˜ì–´ ìˆê³  í”¼ì‚¬ì²´ê°€ ì²œì²œíˆ ì›€ì§ì´ë„ë¡ ì—°ì¶œ 'ë¡œìš° ëª¨ì…˜(Low Motion)' ì¤‘ì—ì„œ ì„ íƒ ê°€ëŠ¥\n",
      "- ì›¹ ì „ìš© ëª¨ë¸ì¸ V1ì€ 1íšŒ ë™ì˜ìƒ ìƒì„± ì‘ì—…ìœ¼ë¡œ 5ì´ˆ ê¸¸ì´ì˜ ë™ì˜ìƒ 4ê°œë¥¼ ì œì‘í•˜ë©°, ìƒì„±ëœ ë™ì˜ìƒì€ í•œ ë²ˆì— 4ì´ˆì”© ìµœëŒ€ 4íšŒê¹Œì§€ ì˜ìƒ ê¸¸ì´ë¥¼ í™•ì¥í•  ìˆ˜ ìˆë„ë¡ í—ˆìš©</content><metadata><source>docling_outputs\\SPRi AI Brief_8ì›”í˜¸_ì‚°ì—…ë™í–¥_F.md</source><page></page></metadata></document>\n",
      "\n",
      "<document><content><!-- image -->\n",
      "\n",
      "## ê¸°ì—… ï½¥ ì‚°ì—…\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## ë¯¸ë“œì €ë‹ˆ, ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ ìƒì„± AI ëª¨ë¸ 'V1' ì¶œì‹œ\n",
      "\n",
      "## KEY Contents\n",
      "\n",
      "- n ë¯¸ë“œì €ë‹ˆê°€ 1íšŒ ì‘ì—…ìœ¼ë¡œ 5ì´ˆ ê¸¸ì´ì˜ ë™ì˜ìƒ 4ê°œë¥¼ ì œì‘í•  ìˆ˜ ìˆëŠ” ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ 'V1'ì„ ì¶œì‹œí•˜ê³  ì—¬íƒ€ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ë³´ë‹¤ 25ë°° ì´ìƒ ì €ë ´í•œ ê°€ê²©ì„ ì±…ì •í–ˆë‹¤ê³  ê°•ì¡°\n",
      "- n ë¯¸ë“œì €ë‹ˆëŠ” ì‹¤ì‹œê°„ ì˜¤í”ˆì›”ë“œ ì‹œë®¬ë ˆì´ì…˜ì´ ê°€ëŠ¥í•œ AI ëª¨ë¸ ê°œë°œì„ ê¶ê·¹ì  ëª©í‘œë¡œ ì œì‹œí•˜ê³ , 2026ë…„ì— 3D ëª¨ë¸ê³¼ ì‹¤ì‹œê°„ ì²˜ë¦¬ ëª¨ë¸ì„ ì¶œì‹œí•œ ë’¤ ê° ëª¨ë¸ì„ í•˜ë‚˜ë¡œ í†µí•©í•  ê³„íšì´ë¼ê³  ì„¤ëª…\n",
      "\n",
      "## Â£ 'V1', ë¯¸ë“œì €ë‹ˆì—ì„œ ìƒì„±í•œ ì´ë¯¸ì§€ë‚˜ ì™¸ë¶€ ì´ë¯¸ì§€ë¥¼ ë™ì˜ìƒìœ¼ë¡œ ë³€í™˜</content><metadata><source>docling_outputs\\SPRi AI Brief_8ì›”í˜¸_ì‚°ì—…ë™í–¥_F.md</source><page></page></metadata></document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mRetriever\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ë¯¸ë“œì €ë‹ˆ ì‹ ë²„ì „ ê´€ë ¨ ìµœì‹  ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "- ë¯¸ë“œì €ë‹ˆê°€ 2025ë…„ 6ì›” 19ì¼ì— ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ 'V1'ì„ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤.\n",
      "- 'V1'ì€ ì´ë¯¸ì§€ë¥¼ ë™ì˜ìƒìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸ë¡œ, ë¯¸ë“œì €ë‹ˆ í”Œë«í¼ì—ì„œ ì œì‘ëœ ì´ë¯¸ì§€ë‚˜ ì™¸ë¶€ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë™ì˜ìƒì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "- ìë™ ì„¤ì • ì‹œ ëª¨ì…˜ í”„ë¡¬í”„íŠ¸ê°€ ìë™ ìƒì„±ë˜ê³ , ìˆ˜ë™ ì„¤ì • ì‹œ ì‚¬ìš©ìê°€ ì‚¬ë¬¼ì´ë‚˜ ì¥ë©´ì˜ ì›€ì§ì„ì„ ì§ì ‘ ì§€ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ì‚¬ìš©ìëŠ” 'í•˜ì´ ëª¨ì…˜(High Motion)'ê³¼ 'ë¡œìš° ëª¨ì…˜(Low Motion)' ì¤‘ì—ì„œ ì›€ì§ì„ ê°•ë„ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ì›¹ ì „ìš© ëª¨ë¸ì¸ V1ì€ 1íšŒ ì‘ì—…ìœ¼ë¡œ 5ì´ˆ ê¸¸ì´ì˜ ë™ì˜ìƒ 4ê°œë¥¼ ì œì‘í•˜ë©°, ìƒì„±ëœ ë™ì˜ìƒì€ ìµœëŒ€ 4íšŒê¹Œì§€ 4ì´ˆì”© ì—°ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ë¯¸ë“œì €ë‹ˆëŠ” ì´ ëª¨ë¸ì„ ì—¬íƒ€ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ë³´ë‹¤ 25ë°° ì´ìƒ ì €ë ´í•œ ê°€ê²©ì— ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "- ê¶ê·¹ì ìœ¼ë¡œ 2026ë…„ì— 3D ëª¨ë¸ê³¼ ì‹¤ì‹œê°„ ì²˜ë¦¬ ëª¨ë¸ì„ ì¶œì‹œí•˜ê³ , ì´ë¥¼ í•˜ë‚˜ë¡œ í†µí•©í•˜ëŠ” ì‹¤ì‹œê°„ ì˜¤í”ˆì›”ë“œ ì‹œë®¬ë ˆì´ì…˜ AI ëª¨ë¸ ê°œë°œì„ ëª©í‘œë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì¶œì²˜: SPRI AI Brief 8ì›”í˜¸ ì‚°ì—…ë™í–¥ (2025)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mRetriever\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: retriever\n",
      "\n",
      "ë¯¸ë“œì €ë‹ˆ ì‹ ë²„ì „ ê´€ë ¨ ìµœì‹  ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "- ë¯¸ë“œì €ë‹ˆê°€ 2025ë…„ 6ì›” 19ì¼ì— ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ 'V1'ì„ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤.\n",
      "- 'V1'ì€ ì´ë¯¸ì§€ë¥¼ ë™ì˜ìƒìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸ë¡œ, ë¯¸ë“œì €ë‹ˆ í”Œë«í¼ì—ì„œ ì œì‘ëœ ì´ë¯¸ì§€ë‚˜ ì™¸ë¶€ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë™ì˜ìƒì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "- ìë™ ì„¤ì • ì‹œ ëª¨ì…˜ í”„ë¡¬í”„íŠ¸ê°€ ìë™ ìƒì„±ë˜ê³ , ìˆ˜ë™ ì„¤ì • ì‹œ ì‚¬ìš©ìê°€ ì‚¬ë¬¼ì´ë‚˜ ì¥ë©´ì˜ ì›€ì§ì„ì„ ì§ì ‘ ì§€ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ì‚¬ìš©ìëŠ” 'í•˜ì´ ëª¨ì…˜(High Motion)'ê³¼ 'ë¡œìš° ëª¨ì…˜(Low Motion)' ì¤‘ì—ì„œ ì›€ì§ì„ ê°•ë„ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ì›¹ ì „ìš© ëª¨ë¸ì¸ V1ì€ 1íšŒ ì‘ì—…ìœ¼ë¡œ 5ì´ˆ ê¸¸ì´ì˜ ë™ì˜ìƒ 4ê°œë¥¼ ì œì‘í•˜ë©°, ìƒì„±ëœ ë™ì˜ìƒì€ ìµœëŒ€ 4íšŒê¹Œì§€ 4ì´ˆì”© ì—°ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ë¯¸ë“œì €ë‹ˆëŠ” ì´ ëª¨ë¸ì„ ì—¬íƒ€ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ë³´ë‹¤ 25ë°° ì´ìƒ ì €ë ´í•œ ê°€ê²©ì— ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "- ê¶ê·¹ì ìœ¼ë¡œ 2026ë…„ì— 3D ëª¨ë¸ê³¼ ì‹¤ì‹œê°„ ì²˜ë¦¬ ëª¨ë¸ì„ ì¶œì‹œí•˜ê³ , ì´ë¥¼ í•˜ë‚˜ë¡œ í†µí•©í•˜ëŠ” ì‹¤ì‹œê°„ ì˜¤í”ˆì›”ë“œ ì‹œë®¬ë ˆì´ì…˜ AI ëª¨ë¸ ê°œë°œì„ ëª©í‘œë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì¶œì²˜: SPRI AI Brief 8ì›”í˜¸ ì‚°ì—…ë™í–¥ (2025)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mnext\u001b[0m:\n",
      "FINISH\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ìˆ˜ì •ëœ ì½”ë“œ í…ŒìŠ¤íŠ¸\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": 123})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"ë¯¸ë“œì €ë‹ˆ ì‹ ë²„ì „ì€? SPRIì—ì„œ ì°¾ì•„ì¤˜\"\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(graph, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fed566b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mnext\u001b[0m:\n",
      "General LLM\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mGeneral LLM\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ì´ì „ ì§ˆë¬¸ì€ \"ë¯¸ë“œì €ë‹ˆ ì‹ ë²„ì „ì€? SPRIì—ì„œ ì°¾ì•„ì¤˜\"ì˜€ìŠµë‹ˆë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mGeneral LLM\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: general\n",
      "\n",
      "ì´ì „ ì§ˆë¬¸ì€ \"ë¯¸ë“œì €ë‹ˆ ì‹ ë²„ì „ì€? SPRIì—ì„œ ì°¾ì•„ì¤˜\"ì˜€ìŠµë‹ˆë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mnext\u001b[0m:\n",
      "FINISH\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ìˆ˜ì •ëœ ì½”ë“œ í…ŒìŠ¤íŠ¸\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": 123})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"ì´ì „ ë‚˜ì˜ ì§ˆë¬¸ì€?\"\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(graph, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06e05ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mnext\u001b[0m:\n",
      "FINISH\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ìˆ˜ì •ëœ ì½”ë“œ í…ŒìŠ¤íŠ¸\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph, stream_graph\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": 123})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"ë¯¸ë“œì €ë‹ˆ ì‹ ë²„ì „\"\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(graph, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b27632b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"next\":\"FINISH\"}"
     ]
    }
   ],
   "source": [
    "# ìˆ˜ì •ëœ ì½”ë“œ í…ŒìŠ¤íŠ¸\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph, stream_graph\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": 123})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"ì•ˆë…•??\"\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(graph, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8780d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a40cc6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì‹¤ì œ ì±—ë´‡ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜ ===\n",
      "ê°™ì€ ì§ˆë¬¸ì´ ì—¬ëŸ¬ ë²ˆ ë“¤ì–´ì™€ë„ ë§¤ë²ˆ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
      "\n",
      "1ï¸âƒ£ ì²« ë²ˆì§¸ ì§ˆë¬¸:\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mnext\u001b[0m:\n",
      "General LLM\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mGeneral LLM\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ì•ˆë…•! ì–´ë–»ê²Œ ë„ì™€ì¤„ê¹Œ?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mGeneral LLM\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: general\n",
      "\n",
      "ì•ˆë…•! ì–´ë–»ê²Œ ë„ì™€ì¤„ê¹Œ?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mnext\u001b[0m:\n",
      "General LLM\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mGeneral LLM\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ì•ˆë…•! ë¬´ì—‡ì„ ë„ì™€ì¤„ê¹Œ? ê¶ê¸ˆí•œ ì ì´ë‚˜ í•„ìš”í•œ ê²Œ ìˆìœ¼ë©´ ë§í•´ì¤˜!\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mGeneral LLM\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: general\n",
      "\n",
      "ì•ˆë…•! ë¬´ì—‡ì„ ë„ì™€ì¤„ê¹Œ? ê¶ê¸ˆí•œ ì ì´ë‚˜ í•„ìš”í•œ ê²Œ ìˆìœ¼ë©´ ë§í•´ì¤˜!\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mnext\u001b[0m:\n",
      "General LLM\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mGeneral LLM\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ì•ˆë…•! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ. ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ì¤„ê¹Œ?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mGeneral LLM\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: general\n",
      "\n",
      "ì•ˆë…•! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ. ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ì¤„ê¹Œ?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mnext\u001b[0m:\n",
      "General LLM\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mGeneral LLM\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mGeneral LLM\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: general\n",
      "\n",
      "ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mSupervisor\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mnext\u001b[0m:\n",
      "General LLM\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mGeneral LLM\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ê¶ê¸ˆí•œ ì ì´ë‚˜ í•„ìš”í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mGeneral LLM\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: general\n",
      "\n",
      "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ê¶ê¸ˆí•œ ì ì´ë‚˜ í•„ìš”í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 10 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGraphRecursionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     10\u001b[39m config1 = RunnableConfig(recursion_limit=\u001b[32m10\u001b[39m, configurable={\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: random_uuid()})\n\u001b[32m     11\u001b[39m inputs1 = {\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     13\u001b[39m         HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mì•ˆë…•?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m     ]\n\u001b[32m     15\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m result1 = \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# ë‘ ë²ˆì§¸ ì§ˆë¬¸ (ê°™ì€ ë‚´ìš©)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\skyop\\jaeho_template\\dotenv\\Lib\\site-packages\\langchain_teddynote\\messages.py:409\u001b[39m, in \u001b[36minvoke_graph\u001b[39m\u001b[34m(graph, inputs, config, node_names, callback)\u001b[39m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[-\u001b[32m1\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mroot graph\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# subgraphs=True ë¥¼ í†µí•´ ì„œë¸Œê·¸ë˜í”„ì˜ ì¶œë ¥ë„ í¬í•¨\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    411\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# node_namesê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í•„í„°ë§\u001b[39;49;00m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnode_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_names\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\skyop\\jaeho_template\\dotenv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2343\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   2334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop.status == \u001b[33m\"\u001b[39m\u001b[33mout_of_steps\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2335\u001b[39m     msg = create_error_message(\n\u001b[32m   2336\u001b[39m         message=(\n\u001b[32m   2337\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m reached \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2341\u001b[39m         error_code=ErrorCode.GRAPH_RECURSION_LIMIT,\n\u001b[32m   2342\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2343\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[32m   2344\u001b[39m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[32m   2345\u001b[39m run_manager.on_chain_end(loop.output)\n",
      "\u001b[31mGraphRecursionError\u001b[39m: Recursion limit of 10 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "# ì‹¤ì œ ì±—ë´‡ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜ - ê°™ì€ ì§ˆë¬¸ ë°˜ë³µ í…ŒìŠ¤íŠ¸\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "print(\"=== ì‹¤ì œ ì±—ë´‡ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜ ===\")\n",
    "print(\"ê°™ì€ ì§ˆë¬¸ì´ ì—¬ëŸ¬ ë²ˆ ë“¤ì–´ì™€ë„ ë§¤ë²ˆ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì§ˆë¬¸\n",
    "print(\"\\n1ï¸âƒ£ ì²« ë²ˆì§¸ ì§ˆë¬¸:\")\n",
    "config1 = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "inputs1 = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"ì•ˆë…•?\")\n",
    "    ]\n",
    "}\n",
    "result1 = invoke_graph(graph, inputs1, config1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# ë‘ ë²ˆì§¸ ì§ˆë¬¸ (ê°™ì€ ë‚´ìš©)\n",
    "print(\"\\n2ï¸âƒ£ ë‘ ë²ˆì§¸ ì§ˆë¬¸ (ê°™ì€ ë‚´ìš©):\")\n",
    "config2 = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "inputs2 = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"ì•ˆë…•?\")\n",
    "    ]\n",
    "}\n",
    "result2 = invoke_graph(graph, inputs2, config2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# ì„¸ ë²ˆì§¸ ì§ˆë¬¸ (ê°™ì€ ë‚´ìš©)\n",
    "print(\"\\n3ï¸âƒ£ ì„¸ ë²ˆì§¸ ì§ˆë¬¸ (ê°™ì€ ë‚´ìš©):\")\n",
    "config3 = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "inputs3 = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"ì•ˆë…•?\")\n",
    "    ]\n",
    "}\n",
    "result3 = invoke_graph(graph, inputs3, config3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d097ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ ì—†ëŠ” ê·¸ë˜í”„ë¡œ í…ŒìŠ¤íŠ¸ (ê° ì§ˆë¬¸ì´ ì™„ì „íˆ ë…ë¦½ì )\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì—†ëŠ” ê·¸ë˜í”„ ìƒì„±\n",
    "workflow_no_memory = StateGraph(AgentState)\n",
    "\n",
    "# ê·¸ë˜í”„ì— ë…¸ë“œ ì¶”ê°€\n",
    "workflow_no_memory.add_node(\"Retriever\", retriever_node)\n",
    "workflow_no_memory.add_node(\"Researcher\", research_node)\n",
    "workflow_no_memory.add_node(\"Coder\", coder_node)\n",
    "workflow_no_memory.add_node(\"General LLM\", general_node)\n",
    "workflow_no_memory.add_node(\"Supervisor\", supervisor_agent)\n",
    "\n",
    "# ë©¤ë²„ ë…¸ë“œ > Supervisor ë…¸ë“œë¡œ ì—£ì§€ ì¶”ê°€\n",
    "for member in members:\n",
    "    workflow_no_memory.add_edge(member, \"Supervisor\")\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "\n",
    "def get_next(state):\n",
    "    return state[\"next\"]\n",
    "\n",
    "# Supervisor ë…¸ë“œì—ì„œ ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "workflow_no_memory.add_conditional_edges(\"Supervisor\", get_next, conditional_map)\n",
    "\n",
    "# ì‹œì‘ì \n",
    "workflow_no_memory.add_edge(START, \"Supervisor\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì—†ëŠ” ê·¸ë˜í”„ ì»´íŒŒì¼ (ìˆ˜ì •ëœ general_node ë°˜ì˜)\n",
    "graph_no_memory = workflow_no_memory.compile()  # checkpointer ì—†ìŒ\n",
    "\n",
    "print(\"=== ë©”ëª¨ë¦¬ ì—†ëŠ” ê·¸ë˜í”„ ì¬ìƒì„± ì™„ë£Œ ===\")\n",
    "print(\"ì´ì œ ê° ì§ˆë¬¸ì´ ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\")\n",
    "print(\"general_nodeë„ react_agentë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬´í•œë£¨í”„ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba294dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ ì—†ëŠ” ê·¸ë˜í”„ë¡œ ê°™ì€ ì§ˆë¬¸ ë°˜ë³µ í…ŒìŠ¤íŠ¸\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "print(\"=== ë©”ëª¨ë¦¬ ì—†ëŠ” ê·¸ë˜í”„ë¡œ ê°™ì€ ì§ˆë¬¸ ë°˜ë³µ í…ŒìŠ¤íŠ¸ ===\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì§ˆë¬¸\n",
    "print(\"\\n1ï¸âƒ£ ì²« ë²ˆì§¸ ì§ˆë¬¸:\")\n",
    "inputs1 = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"ì•ˆë…•?\")\n",
    "    ]\n",
    "}\n",
    "result1 = invoke_graph(graph_no_memory, inputs1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# ë‘ ë²ˆì§¸ ì§ˆë¬¸ (ê°™ì€ ë‚´ìš©)\n",
    "print(\"\\n2ï¸âƒ£ ë‘ ë²ˆì§¸ ì§ˆë¬¸ (ê°™ì€ ë‚´ìš©):\")\n",
    "inputs2 = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"ì•ˆë…•?\")\n",
    "    ]\n",
    "}\n",
    "result2 = invoke_graph(graph_no_memory, inputs2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# ì„¸ ë²ˆì§¸ ì§ˆë¬¸ (ê°™ì€ ë‚´ìš©)\n",
    "print(\"\\n3ï¸âƒ£ ì„¸ ë²ˆì§¸ ì§ˆë¬¸ (ê°™ì€ ë‚´ìš©):\")\n",
    "inputs3 = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"ì•ˆë…•?\")\n",
    "    ]\n",
    "}\n",
    "result3 = invoke_graph(graph_no_memory, inputs3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e70c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ì •ëœ general_node í…ŒìŠ¤íŠ¸ (ë¬´í•œë£¨í”„ ë°©ì§€)\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "print(\"=== ìˆ˜ì •ëœ general_node í…ŒìŠ¤íŠ¸ ===\")\n",
    "print(\"ì´ì œ general_nodeë„ react_agentë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬´í•œë£¨í”„ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ìƒˆë¡œìš´ thread_idë¡œ config ì„¤ì •\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"ì•ˆë…•?\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ìˆ˜ì •ëœ ê·¸ë˜í”„ë¡œ ì‹¤í–‰\n",
    "result = invoke_graph(graph, inputs, config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dotenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
