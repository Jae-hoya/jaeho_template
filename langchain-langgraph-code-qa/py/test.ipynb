{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrievers_reranker import init_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = init_retriever(db_index=\"../LANGCHAIN_DB_INDEX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\skyop\\\\JaehoNote\\\\fastcampus-code-qa_copy/langgraph-main\\\\docs\\\\docs\\\\tutorials\\\\rag\\\\langgraph_adaptive_rag.ipynb', 'relevance_score': 0.3638729155063629}, page_content='# Adaptive RAG\\n\\nAdaptive RAG is a strategy for RAG that unites (1) [query analysis](https://blog.langchain.dev/query-construction/) with (2) [active / self-corrective RAG](https://blog.langchain.dev/agentic-rag-with-langgraph/).\\n\\nIn the [paper](https://arxiv.org/abs/2403.14403), they report query analysis to route across:\\n\\n* No Retrieval\\n* Single-shot RAG\\n* Iterative RAG\\n\\nLet\\'s build on this using LangGraph. \\n\\nIn our implementation, we will route between:\\n\\n* Web search: for questions related to recent events\\n* Self-corrective RAG: for questions related to our index\\n\\n![Screenshot 2024-03-26 at 1.36.03 PM.png](attachment:36fa621a-9d3d-4860-a17c-5d20e6987481.png)\\n\\n## Setup\\n\\nFirst, let\\'s install our required packages and set our API keys\\n\\n```python\\n%%capture --no-stderr\\n%pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\\n```\\n\\n```python\\nimport getpass\\nimport os\\n\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"OPENAI_API_KEY\")\\n# _set_env(\"COHERE_API_KEY\")\\n_set_env(\"TAVILY_API_KEY\")\\n```\\n\\n<div class=\"admonition tip\">\\n    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\\n    <p style=\"padding-top: 5px;\">\\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \\n    </p>\\n</div>\\n\\n## Create Index\\n\\n```python\\n### Build Index\\n\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_openai import OpenAIEmbeddings\\n\\n### from langchain_cohere import CohereEmbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\skyop\\\\JaehoNote\\\\fastcampus-code-qa_copy/langgraph-main\\\\examples\\\\rag\\\\langgraph_adaptive_rag.ipynb', 'relevance_score': 0.3593641221523285}, page_content='# Adaptive RAG\\n\\nAdaptive RAG is a strategy for RAG that unites (1) [query analysis](https://blog.langchain.dev/query-construction/) with (2) [active / self-corrective RAG](https://blog.langchain.dev/agentic-rag-with-langgraph/).\\n\\nIn the [paper](https://arxiv.org/abs/2403.14403), they report query analysis to route across:\\n\\n* No Retrieval\\n* Single-shot RAG\\n* Iterative RAG\\n\\nLet\\'s build on this using LangGraph. \\n\\nIn our implementation, we will route between:\\n\\n* Web search: for questions related to recent events\\n* Self-corrective RAG: for questions related to our index\\n\\n![Screenshot 2024-03-26 at 1.36.03 PM.png](attachment:36fa621a-9d3d-4860-a17c-5d20e6987481.png)\\n\\n## Setup\\n\\nFirst, let\\'s install our required packages and set our API keys\\n\\n```python\\n%%capture --no-stderr\\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\\n```\\n\\n```python\\nimport getpass\\nimport os\\n\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"OPENAI_API_KEY\")\\n_set_env(\"COHERE_API_KEY\")\\n_set_env(\"TAVILY_API_KEY\")\\n```\\n\\n<div class=\"admonition tip\">\\n    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\\n    <p style=\"padding-top: 5px;\">\\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \\n    </p>\\n</div>    \\n\\n## Create Index\\n\\n```python\\n### Build Index\\n\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_openai import OpenAIEmbeddings\\n\\n### from langchain_cohere import CohereEmbeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\skyop\\\\JaehoNote\\\\fastcampus-code-qa_copy/langgraph-main\\\\examples\\\\rag\\\\langgraph_adaptive_rag_cohere.ipynb', 'relevance_score': 0.3407045006752014}, page_content='# RAG generation\\n    generation = rag_chain.invoke({\"documents\": documents, \"question\": question})\\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\\n\\n\\ndef grade_documents(state):\\n    \"\"\"\\n    Determines whether the retrieved documents are relevant to the question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with only filtered relevant documents\\n    \"\"\"\\n\\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Score each doc\\n    filtered_docs = []\\n    for d in documents:\\n        score = retrieval_grader.invoke(\\n            {\"question\": question, \"document\": d.page_content}\\n        )\\n        grade = score.binary_score\\n        if grade == \"yes\":\\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\\n            filtered_docs.append(d)\\n        else:\\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\\n            continue\\n    return {\"documents\": filtered_docs, \"question\": question}\\n\\n\\ndef web_search(state):\\n    \"\"\"\\n    Web search based on the re-phrased question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with appended web results\\n    \"\"\"\\n\\n    print(\"---WEB SEARCH---\")\\n    question = state[\"question\"]\\n\\n    # Web search\\n    docs = web_search_tool.invoke({\"query\": question})\\n    web_results = \"\\\\n\".join([d[\"content\"] for d in docs])\\n    web_results = Document(page_content=web_results)\\n\\n    return {\"documents\": web_results, \"question\": question}\\n\\n\\n### Edges ###\\n\\n\\ndef route_question(state):\\n    \"\"\"\\n    Route question to web search or RAG.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Next node to call\\n    \"\"\"\\n\\n    print(\"---ROUTE QUESTION---\")\\n    question = state[\"question\"]\\n    source = question_router.invoke({\"question\": question})'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\skyop\\\\JaehoNote\\\\fastcampus-code-qa_copy\\\\langgraph-main\\\\docs\\\\docs\\\\tutorials\\\\index.md', 'relevance_score': 0.32167303562164307}, page_content=\"### RAG\\n\\n- [Agentic RAG](rag/langgraph_agentic_rag.ipynb): Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\\n- [Adaptive RAG](rag/langgraph_adaptive_rag.ipynb): Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\\n    - For a version that uses a local LLM: [Adaptive RAG using local LLMs](rag/langgraph_adaptive_rag_local.ipynb)\\n- [Corrective RAG](rag/langgraph_crag.ipynb): Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \\n    - For a version that uses a local LLM: [Corrective RAG using local LLMs](rag/langgraph_crag_local.ipynb)\\n- [Self-RAG](rag/langgraph_self_rag.ipynb): Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\\n    - For a version that uses a local LLM: [Self-RAG using local LLMs](rag/langgraph_self_rag_local.ipynb) \\n- [SQL Agent](sql-agent.ipynb): Build a SQL agent that can answer questions about a SQL database.\\n\\n\\n### Agent Architectures\\n\\n#### Multi-Agent Systems\\n\\n- [Network](multi_agent/multi-agent-collaboration.ipynb): Enable two or more agents to collaborate on a task\\n- [Supervisor](multi_agent/agent_supervisor.ipynb): Use an LLM to orchestrate and delegate to individual agents\\n- [Hierarchical Teams](multi_agent/hierarchical_agent_teams.ipynb): Orchestrate nested teams of agents to solve problems\\n \\n#### Planning Agents\"),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\skyop\\\\JaehoNote\\\\fastcampus-code-qa_copy/langgraph-main\\\\examples\\\\rag\\\\langgraph_adaptive_rag_cohere.ipynb', 'relevance_score': 0.2934044599533081}, page_content='# Adaptive RAG Cohere Command R\\n\\nAdaptive RAG is a strategy for RAG that unites (1) [query analysis](https://blog.langchain.dev/query-construction/) with (2) [active / self-corrective RAG](https://blog.langchain.dev/agentic-rag-with-langgraph/).\\n\\nIn the paper, they report query analysis to route across:\\n\\n* No Retrieval (LLM answers)\\n* Single-shot RAG\\n* Iterative RAG\\n\\nLet\\'s build on this to perform query analysis to route across some more interesting cases:\\n\\n* No Retrieval (LLM answers)\\n* Web-search\\n* Iterative RAG\\n\\nWe\\'ll use [Command R](https://cohere.com/blog/command-r), a recent release from Cohere that:\\n\\n* Has strong accuracy on RAG and Tool Use\\n* Has 128k context\\n* Has low latency \\n  \\n![Screenshot 2024-04-02 at 8.11.18 PM.png](attachment:2a4ecdd2-280d-4311-a2cd-cd6138090be9.png)\\n\\n# Environment\\n\\n```python\\n! pip install --quiet langchain langchain_cohere langchain-openai tiktoken langchainhub chromadb langgraph\\n```\\n\\n```python\\n### LLMs\\nimport os\\n\\nos.environ[\"COHERE_API_KEY\"] = \"<your-api-key>\"\\n```\\n\\n```python\\n# ### Tracing (optional)\\n# os.environ[\\'LANGCHAIN_TRACING_V2\\'] = \\'true\\'\\n# os.environ[\\'LANGCHAIN_ENDPOINT\\'] = \\'https://api.smith.langchain.com\\'\\n# os.environ[\\'LANGCHAIN_API_KEY\\'] =\\'<your-api-key>\\'\\n```\\n\\n## Index\\n\\n```python\\n### Build Index\\n\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_cohere import CohereEmbeddings\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import Chroma\\n\\n# Set embeddings\\nembd = CohereEmbeddings()\\n\\n# Docs to index\\nurls = [\\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\\n]\\n\\n# Load\\ndocs = [WebBaseLoader(url).load() for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\skyop\\\\JaehoNote\\\\fastcampus-code-qa_copy/langgraph-main\\\\examples\\\\rag\\\\langgraph_adaptive_rag_local.ipynb', 'relevance_score': 0.23651622235774994}, page_content='# Adaptive RAG using local LLMs\\n\\nAdaptive RAG is a strategy for RAG that unites (1) [query analysis](https://blog.langchain.dev/query-construction/) with (2) [active / self-corrective RAG](https://blog.langchain.dev/agentic-rag-with-langgraph/).\\n\\nIn the [paper](https://arxiv.org/abs/2403.14403), they report query analysis to route across:\\n\\n* No Retrieval\\n* Single-shot RAG\\n* Iterative RAG\\n\\nLet\\'s build on this using LangGraph. \\n\\nIn our implementation, we will route between:\\n\\n* Web search: for questions related to recent events\\n* Self-corrective RAG: for questions related to our index\\n\\n![Screenshot 2024-04-01 at 1.29.15 PM.png](attachment:3755396d-c4a8-45bd-87d4-00cb56339fe5.png)\\n\\n## Setup\\n\\nFirst, let\\'s install our required packages and set our API keys\\n\\n```python\\n%capture --no-stderr\\n%pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python nomic[local]\\n```\\n\\n```python\\nimport getpass\\nimport os\\n\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"TAVILY_API_KEY\")\\n_set_env(\"NOMIC_API_KEY\")\\n```\\n\\n<div class=\"admonition tip\">\\n    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\\n    <p style=\"padding-top: 5px;\">\\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \\n    </p>\\n</div>    \\n\\n### LLMs\\n\\n#### Local Embeddings\\n\\nYou can use `GPT4AllEmbeddings()` from Nomic, which can access use Nomic\\'s recently released [v1](https://blog.nomic.ai/posts/nomic-embed-text-v1) and [v1.5](https://blog.nomic.ai/posts/nomic-embed-matryoshka) embeddings.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\skyop\\\\JaehoNote\\\\fastcampus-code-qa_copy/langgraph-main\\\\docs\\\\docs\\\\tutorials\\\\rag\\\\langgraph_adaptive_rag_local.ipynb', 'relevance_score': 0.23651622235774994}, page_content='See conceptual docs [here](https://langchain-ai.github.io/langgraph/concepts/low_level/#edges).\\n\\n```python\\nfrom langchain.schema import Document\\nfrom langgraph.graph import END\\n\\n\\n### Nodes\\ndef retrieve(state):\\n    \"\"\"\\n    Retrieve documents from vectorstore\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, documents, that contains retrieved documents\\n    \"\"\"\\n    print(\"---RETRIEVE---\")\\n    question = state[\"question\"]\\n\\n    # Write retrieved documents to documents key in state\\n    documents = retriever.invoke(question)\\n    return {\"documents\": documents}\\n\\n\\ndef generate(state):\\n    \"\"\"\\n    Generate answer using RAG on retrieved documents\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, generation, that contains LLM generation\\n    \"\"\"\\n    print(\"---GENERATE---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n    loop_step = state.get(\"loop_step\", 0)\\n\\n    # RAG generation\\n    docs_txt = format_docs(documents)\\n    rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\\n    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\\n    return {\"generation\": generation, \"loop_step\": loop_step + 1}\\n\\n\\ndef grade_documents(state):\\n    \"\"\"\\n    Determines whether the retrieved documents are relevant to the question\\n    If any document is not relevant, we will set a flag to run web search\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Filtered out irrelevant documents and updated web_search state\\n    \"\"\"\\n\\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\skyop\\\\JaehoNote\\\\fastcampus-code-qa_copy\\\\langgraph-main\\\\docs\\\\docs\\\\tutorials\\\\workflows\\\\index.md', 'relevance_score': 0.22000710666179657}, page_content='[Here](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag_local/) is a RAG workflow that grades answers for hallucinations or errors. See our video [here](https://www.youtube.com/watch?v=bq1Plo2RhYI).\\n\\n=== \"Functional API\"\\n\\n    ```python\\n    # Schema for structured output to use in evaluation\\n    class Feedback(BaseModel):\\n        grade: Literal[\"funny\", \"not funny\"] = Field(\\n            description=\"Decide if the joke is funny or not.\",\\n        )\\n        feedback: str = Field(\\n            description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n        )\\n\\n\\n    # Augment the LLM with schema for structured output\\n    evaluator = llm.with_structured_output(Feedback)\\n\\n\\n    # Nodes\\n    @task\\n    def llm_call_generator(topic: str, feedback: Feedback):\\n        \"\"\"LLM generates a joke\"\"\"\\n        if feedback:\\n            msg = llm.invoke(\\n                f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\\n            )\\n        else:\\n            msg = llm.invoke(f\"Write a joke about {topic}\")\\n        return msg.content\\n\\n\\n    @task\\n    def llm_call_evaluator(joke: str):\\n        \"\"\"LLM evaluates the joke\"\"\"\\n        feedback = evaluator.invoke(f\"Grade the joke {joke}\")\\n        return feedback\\n\\n\\n    @entrypoint()\\n    def optimizer_workflow(topic: str):\\n        feedback = None\\n        while True:\\n            joke = llm_call_generator(topic, feedback).result()\\n            feedback = llm_call_evaluator(joke).result()\\n            if feedback.grade == \"funny\":\\n                break\\n\\n        return joke\\n\\n    # Invoke\\n    for step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\\n        print(step)\\n        print(\"\\\\n\")\\n    ```\\n\\n    **LangSmith Trace**\\n\\n    https://smith.langchain.com/public/f66830be-4339-4a6b-8a93-389ce5ae27b4/r\\n\\n## Agent')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Adaptive-RAG에서 문서관령성에 대해 설명해 주세요\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
