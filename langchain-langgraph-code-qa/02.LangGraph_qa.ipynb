{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848dbd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50776c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore, FastEmbedSparse, RetrievalMode\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "dense_embedding = OllamaEmbeddings(model=\"bge-m3\")\n",
    "sparse_embedding = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "qdrant = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"LangChain_LangGraph_QA\",\n",
    "    embedding = dense_embedding,\n",
    "    sparse_embedding = sparse_embedding,\n",
    "    retrieval_mode = RetrievalMode.HYBRID,\n",
    "    vector_name= \"dense\",\n",
    "    sparse_vector_name = \"sparse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3527d2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Self RAG\n",
      "\n",
      "Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. \n",
      "\n",
      "[Paper](https://arxiv.org/abs/2310.11511)\n",
      "\n",
      "![Screenshot 2024-04-01 at 12.41.50 PM.png](attachment:15cba0ab-a549-4909-8373-fb761e384eff.png)\n",
      "\n",
      "# Environment \n",
      "\n",
      "```python\n",
      "%pip install -qU langchain-pinecone langchain-openai langchainhub langgraph\n",
      "```\n",
      "\n",
      "### Tracing\n",
      "\n",
      "Use [LangSmith](https://docs.smith.langchain.com/) for tracing (shown at bottom)\n",
      "\n",
      "```python\n",
      "import os\n",
      "\n",
      "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
      "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
      "os.environ[\"LANGCHAIN_API_KEY\"] = \"<your-api-key>\"\n",
      "```\n",
      "\n",
      "```python\n",
      "import os\n",
      "\n",
      "os.environ[\"LANGCHAIN_PROJECT\"] = \"pinecone-devconnect\"\n",
      "```\n",
      "\n",
      "## Retriever\n",
      " \n",
      "Let's use Pinecone's sample movies database\n",
      "\n",
      "```python\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "from langchain_pinecone import PineconeVectorStore\n",
      "\n",
      "# use pinecone movies database\n",
      "\n",
      "# Add to vectorDB\n",
      "vectorstore = PineconeVectorStore(\n",
      "    embedding=OpenAIEmbeddings(),\n",
      "    index_name=\"sample-movies\",\n",
      "    text_key=\"summary\",\n",
      ")\n",
      "retriever = vectorstore.as_retriever()\n",
      "```\n",
      "\n",
      "```python\n",
      "docs = retriever.invoke(\"James Cameron\")\n",
      "for doc in docs:\n",
      "    print(\"# \" + doc.metadata[\"title\"])\n",
      "    print(doc.page_content)\n",
      "    print()\n",
      "```\n",
      "\n",
      "## Structured Output - Retrieval Grader\n",
      "\n",
      "```python\n",
      "### Retrieval Grader\n",
      "\n",
      "from langchain import hub\n",
      "from langchain_core.pydantic_v1 import BaseModel, Field\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "\n",
      "# Data model\n",
      "class GradeDocuments(BaseModel):\n",
      "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
      "\n",
      "    binary_score: str = Field(\n",
      "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
      "    )\n",
      "\n",
      "\n",
      "# https://smith.langchain.com/hub/efriis/self-rag-retrieval-grader\n",
      "grade_prompt = hub.pull(\"efriis/self-rag-retrieval-grader\")\n",
      "\n",
      "# LLM with function call\n",
      "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
      "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n"
     ]
    }
   ],
   "source": [
    "retriever = qdrant.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(retriever.invoke(\"self-rag\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a07ac",
   "metadata": {},
   "source": [
    "## RAG 체인 구축\n",
    "\n",
    "여기서는 Naive 형태의 Chain 을 구성하도록 한다.\n",
    "\n",
    "단, Reranker 를 사용하여 검색 정확도를 향상시키도록 한다. (`Jina-reranker` 를 사용하지만, 다른 reranker 를 사용하거나, 빼도 좋다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be9a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import JinaRerank\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "from langchain import hub\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "# retriever 생성\n",
    "code_retriever = qdrant.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "# JinaRerank 설정\n",
    "compressor = JinaRerank(model=\"jina-reranker-v2-base-multilingual\", top_n=8)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=code_retriever\n",
    ")\n",
    "\n",
    "# prompt 설정\n",
    "system_prompt = \"\"\"\n",
    "You are an CODE Copilot Assistant. \n",
    "You must use the following pieces of retrieved source code or documentation to answer the question. \n",
    "You are given question related to RAG(Retrieval Augmented Generation) source code and documentation.\n",
    "If you don't know the answer, just say that you don't know. Answer in Korean.\n",
    "\n",
    "When answering questions, follow these guidelines:\n",
    "\n",
    "1. Use only the information provided in the context. \n",
    "2. Include as many example code snippets as possible.\n",
    "3. Writing a full code snippet is highly recommended.\n",
    "4. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "5. The context contain sources at the topic of each individual document.\n",
    "6. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
    "7. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
    "8. If the source is: <source>assistant/docs/llama3_1.md\" page=\"7\"</source>' then just list: \n",
    "        \n",
    "[1] llama3_1.md\n",
    "        \n",
    "And skip the addition of the brackets as well as the Document source preamble in your citation.\n",
    "\n",
    "----\n",
    "\n",
    "### Sources\n",
    "\n",
    "In the Sources section:\n",
    "- Include all sources used in your answer\n",
    "- Provide full links to relevant websites or document names\n",
    "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
    "- It will look like:\n",
    "\n",
    "**Sources**\n",
    "- [1] Link or Document name\n",
    "- [2] Link or Document name\n",
    "\n",
    "Be sure to combine sources. For example this is not correct:\n",
    "\n",
    "- [3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "- [4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "There should be no redundant sources. It should simply be:\n",
    "\n",
    "- [3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "-----\n",
    "\n",
    "### Retrieved Context\n",
    "\n",
    "Here is the context that you can use to answer the question:\n",
    "\n",
    "{context}\n",
    "\n",
    "----\n",
    "\n",
    "### Question\n",
    "\n",
    "Here is user's question:\n",
    "\n",
    "{question}\n",
    "\n",
    "----\n",
    "\n",
    "Final review:\n",
    "- Ensure the report follows the required structure\n",
    "- Check that all guidelines have been followed\n",
    "- Check if a full code snippet is included in your answer if applicable.\n",
    "- Your response should be written in Korean\n",
    "- Using many example code snippets would be rewarded by the user\n",
    "- Think step by step.\n",
    "\n",
    "----\n",
    "\n",
    "Your answer to the question with the source:\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt\n",
    "        ),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"context\": itemgetter(\"context\"),\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f98e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = rag_chain.invoke({\"question\": \"Adaptive RAG\", \"context\": compression_retriever.invoke(\"Adaptive RAG\")})\n",
    "response = rag_chain.invoke({\"question\": \"Adaptive RAG\", \"context\": retriever.invoke(\"Adaptive RAG\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88365496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive RAG는 RAG(Retrieval Augmented Generation) 전략 중 하나로, 쿼리 분석(query analysis)과 능동적(self-corrective) RAG를 결합한 방식입니다. 이 전략은 쿼리의 특성에 따라 적절한 처리 경로를 선택하여 더 효율적이고 정확한 답변을 생성하는 것을 목표로 합니다.\n",
      "\n",
      "### Adaptive RAG 주요 개념\n",
      "- 쿼리 분석을 통해 질문 유형을 파악하고, 이에 따라 다음 경로 중 하나로 라우팅합니다:\n",
      "  - Retrieval 없이 LLM이 직접 답변하는 경우 (No Retrieval)\n",
      "  - 단일 검색 후 답변 생성 (Single-shot RAG)\n",
      "  - 반복적 검색 및 답변 생성 (Iterative RAG)\n",
      "  - 웹 검색(Web-search)을 통한 보완\n",
      "- 예를 들어, 최신 정보가 필요한 질문은 웹 검색으로, 인덱스 내 정보가 충분한 질문은 RAG로 처리합니다.\n",
      "\n",
      "### Adaptive RAG 구현 예시 (LangGraph 기반)\n",
      "\n",
      "```python\n",
      "# 필요한 패키지 설치\n",
      "! pip install --quiet langchain langchain_cohere langchain-openai tiktoken langchainhub chromadb langgraph\n",
      "\n",
      "import os\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain_cohere import CohereEmbeddings\n",
      "from langchain_community.document_loaders import WebBaseLoader\n",
      "from langchain_community.vectorstores import Chroma\n",
      "\n",
      "# API 키 설정\n",
      "os.environ[\"COHERE_API_KEY\"] = \"<your-cohere-api-key>\"\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"<your-openai-api-key>\"\n",
      "\n",
      "# 문서 URL 리스트\n",
      "urls = [\n",
      "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
      "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
      "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
      "]\n",
      "\n",
      "# 웹에서 문서 로드\n",
      "docs = [WebBaseLoader(url).load() for url in urls]\n",
      "docs_list = [item for sublist in docs for item in sublist]\n",
      "\n",
      "# 텍스트 분할기 설정\n",
      "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
      "split_docs = text_splitter.split_documents(docs_list)\n",
      "\n",
      "# 임베딩 생성기 설정\n",
      "embeddings = CohereEmbeddings()\n",
      "\n",
      "# 벡터 스토어 생성 (Chroma 사용)\n",
      "vectorstore = Chroma.from_documents(split_docs, embeddings)\n",
      "\n",
      "# Adaptive RAG의 쿼리 분석 및 라우팅 로직 예시 (의사 코드)\n",
      "def adaptive_rag(query: str):\n",
      "    # 1. 쿼리 분석 (예: 최신 정보 필요 여부 판단)\n",
      "    if \"최신\" in query or \"최근\" in query:\n",
      "        # 웹 검색 수행\n",
      "        print(\"웹 검색 경로 선택\")\n",
      "        # 웹 검색 API 호출 및 결과 처리\n",
      "    else:\n",
      "        # 인덱스 내 검색 수행\n",
      "        print(\"RAG 경로 선택\")\n",
      "        # 벡터 스토어에서 유사 문서 검색 후 LLM에 전달하여 답변 생성\n",
      "\n",
      "# 예시 쿼리\n",
      "adaptive_rag(\"최신 AI 연구 동향은 무엇인가요?\")\n",
      "adaptive_rag(\"프롬프트 엔지니어링이란 무엇인가요?\")\n",
      "```\n",
      "\n",
      "### 요약\n",
      "- Adaptive RAG는 쿼리 분석을 통해 적절한 검색 및 생성 전략을 선택하는 RAG 방식입니다.\n",
      "- 최신 정보가 필요한 경우 웹 검색을, 그렇지 않은 경우 인덱스 기반 RAG를 사용합니다.\n",
      "- LangGraph와 Cohere Embeddings, Chroma 벡터 스토어 등을 활용해 구현할 수 있습니다.\n",
      "\n",
      "더 자세한 내용과 구현 예시는 아래 문서들을 참고하세요.\n",
      "\n",
      "**Sources**\n",
      "- Adaptive RAG Cohere Command R 예제: langgraph_adaptive_rag_cohere.ipynb  \n",
      "- Adaptive RAG 기본 개념 및 구현: langgraph_adaptive_rag.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70141a6c",
   "metadata": {},
   "source": [
    "## LangGraph 체인 구축을 위한 노드 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422f6b2",
   "metadata": {},
   "source": [
    "### Routing\n",
    "\n",
    "사용자의 질문에 대한 routing 을 수행합니다.\n",
    "\n",
    "질문이 vector 검색이 필요한지 여부를 판단하여 라우팅하며, 최대한 보수적으로 판단하도록 프롬프트를 설정하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4769684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question, determine if it needs to be retrieved from vectorstore or not. Return 'yes' if it needs to be retrieved from vectorstore, otherwise return 'no'.\",\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system = \"\"\"\n",
    "You are an expert at routing a user question. \n",
    "The vectorstore contains documents retrieved using LangChain / LangGraph.\n",
    "The vectorstore contains documents related to RAG(Retrieval Augmented Generation) source code and documentation.\n",
    "Return 'yes' if the question is related to the source code or documentation, otherwise return 'no'.\n",
    "If you can't determine if the question is related to the source code or documentation, return 'yes'.\n",
    "If you don't know the answer, return 'yes'.\n",
    "\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbd4c0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(binary_score='yes')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_router.invoke({\"question\": \"MemorySaver 에 대해 설명하세요.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df101d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(binary_score='no')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_router.invoke({\"question\": \"대한민국의 수도는?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b606f",
   "metadata": {},
   "source": [
    "## Query Rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be4468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "\n",
    "system = \"\"\"\n",
    "You a question re-writer that converts an input question to a better version that is optimized for CODE SEARCH(github repository).\n",
    "\n",
    "Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "\n",
    "Base Code Repository: \n",
    "\n",
    "https://github.com/langchain-ai/langgraph\n",
    "\n",
    "Output should be in English.\"\"\"\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bc6d520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I implement Hierarchical Retrieval-Augmented Generation (RAG) using the LangGraph framework?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_rewriter.invoke(\"Hierarchical RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35452669",
   "metadata": {},
   "source": [
    "## 평가\n",
    "\n",
    "- 검색된 문서의 관련성 평가\n",
    "- 답변의 환각 여부 평가\n",
    "- 답변의 질문에 대한 관련성 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96610d",
   "metadata": {},
   "source": [
    "### 검색된 문서의 관련성 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "784dab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    \n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc3fe4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"self-rag\"\n",
    "# retrieval_grader.invoke({\"question\": question, \"document\": retriever.invoke(question)[0].page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99bf18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel, Field\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# # 문서 평가를 위한 데이터 모델 정의\n",
    "# class GradeDocuments(BaseModel):\n",
    "#     \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "#     binary_score: str = Field(\n",
    "#         description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# # LLM 초기화 및 함수 호출을 통한 구조화된 출력 생성\n",
    "# structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# # 시스템 메시지와 사용자 질문을 포함한 프롬프트 템플릿 생성\n",
    "# system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "#     If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "#     It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "#     Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "# grade_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system),\n",
    "#         (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # 문서 검색결과 평가기 생성\n",
    "# retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6605e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"self-rag\"\n",
    "# retrieval_grader.invoke({\"question\": question, \"document\": compression_retriever.invoke(question)[0].page_content})\n",
    "\n",
    "# question = \"self-rag\"\n",
    "# retrieval_grader.invoke({\"question\": question, \"document\": retriever.invoke(question)[0].page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0859d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e238c81e",
   "metadata": {},
   "source": [
    "### 답변의 환각 여부 평가\n",
    "\n",
    "`groundedness_checker`: 검색과 답변을 비교\n",
    "\n",
    "`relevant_answer_checker`: 질문과 답변을 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e89b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerGroundedness(BaseModel):\n",
    "    \"\"\"Binary score for answer groundedness\"\"\"\n",
    "    \n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is groundedness in the facts(given context), 'yes' or 'no'\"\n",
    "    )\n",
    "    \n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)    \n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(AnswerGroundedness)\n",
    "\n",
    "system = \"\"\"\n",
    "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\n",
    "\"\"\"\n",
    "\n",
    "groundedness_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 답변의 환각 여부 평가기 생성\n",
    "groundedness_checker = groundedness_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "101a6109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerGroundedness(binary_score='yes')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question = \"self-rag\"\n",
    "# groundedness_checker.invoke({\"documents\": compression_retriever.invoke(question), \"generation\": rag_chain.invoke({\"question\": question, \"context\": compression_retriever.invoke(question)})})\n",
    "\n",
    "question = \"self-rag\"\n",
    "groundedness_checker.invoke({\"documents\": retriever.invoke(question), \"generation\": rag_chain.invoke({\"question\": question, \"context\": retriever.invoke(question)})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dde46c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary scoring to evaluate the appropriateness of answers to questions\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicate 'yes' or 'no' whether the answer solves the question\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 함수 호출을 통한 LLM 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# 프롬프트 설정\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "relevant_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 프롬프트 템플릿과 구조화된 LLM 평가기를 결합하여 답변 평가기 생성\n",
    "relevant_answer_checker = relevant_answer_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c544aeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question = \"self-rag\"\n",
    "# relevant_answer_checker.invoke({\"question\": question, \n",
    "#                                 \"generation\": rag_chain.invoke({\"question\": question, \"context\": qdrant.invoke(question)[0].page_content})})\n",
    "\n",
    "question = \"self-rag\"\n",
    "relevant_answer_checker.invoke({\"question\": question, \n",
    "                                \"generation\": rag_chain.invoke({\"question\": question, \"context\": retriever.invoke(question)[0].page_content})})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1395f42",
   "metadata": {},
   "source": [
    "## 도구정의\n",
    "\n",
    "웹서치를 위한 TavilySearch \n",
    "\n",
    "DUCKDUCKGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2cd5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily.tavily_search import TavilySearch\n",
    "\n",
    "tavily_search_tool = TavilySearch(max_result=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93eb7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tavily_search_tool.invoke(\"agent2agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6be860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
    "\n",
    "# duck = DuckDuckGoSearchRun(max_result=3)\n",
    "# print(duck.invoke(\"agent2agent\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9b0cc",
   "metadata": {},
   "source": [
    "## 상태정의\n",
    "\n",
    "`AgentState` :  `messages: Annotated[Sequence[BaseMessage], add_messages]`로 정의한 messages가 추가된 형태."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8cc32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Annotated\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "\n",
    "# class GraphState(TypedDict):\n",
    "#     question: Annotated[str, \"User question\"]\n",
    "#     gneration: Annotated[str, \"LLM generated answer\"]\n",
    "#     documents: Annotated[list[Document], \"List of documents\"]\n",
    "\n",
    "class GraphState(AgentState):\n",
    "    question: Annotated[str, \"User question\"]\n",
    "    generation: Annotated[str, \"LLM generated answer\"]\n",
    "    documents: Annotated[list[Document], \"List of documents\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60139064",
   "metadata": {},
   "source": [
    "## 노드 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7c9d6",
   "metadata": {},
   "source": [
    "### 질문 라우팅 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8351c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question_node(state):\n",
    "    question = state[\"question\"]\n",
    "    evaluation = question_router.invoke({\"question\": question})\n",
    "    \n",
    "    if evaluation.binary_score == \"yes\":\n",
    "        return \"query_expansion\"\n",
    "    else: \n",
    "        return \"general_answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ffa5b",
   "metadata": {},
   "source": [
    "### 질문 재작성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a18924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rewrite_node(state):\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    \n",
    "    return {\"question\": better_question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed1469",
   "metadata": {},
   "source": [
    "### 문서 검색 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60ee3eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_node(state):\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # documents = compression_retriever.invoke(question)\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a695da",
   "metadata": {},
   "source": [
    "### 일반 답변 생성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbdd9068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "def general_answer_node(state):\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "    question = state[\"question\"]\n",
    "    answer = llm.invoke(question)\n",
    "    return {\"generation\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8696fd",
   "metadata": {},
   "source": [
    "### RAG 답변 생성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "832695b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer_node(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    answer = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"generation\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eea462",
   "metadata": {},
   "source": [
    "### 문서 관련성을 평가 후 필터링\n",
    "\n",
    "filtered_docs에 관련성 있는 문서 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c4f1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_documents_node(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    filtered_docs = []\n",
    "    \n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return {\"documents\": filtered_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19bf1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 문서의 관련성을 평가 후 필터링\n",
    "# def filtering_documents_node(state):\n",
    "#     # 질문과 문서 검색 결과 가져오기\n",
    "#     question = state[\"question\"]\n",
    "#     documents = state[\"documents\"]\n",
    "\n",
    "#     # 각 문서에 대한 관련성 점수 계산\n",
    "#     filtered_docs = []\n",
    "#     for d in documents:\n",
    "#         score = retrieval_grader.invoke(\n",
    "#             {\"question\": question, \"document\": d.page_content}\n",
    "#         )\n",
    "#         grade = score.binary_score\n",
    "#         if grade == \"yes\":\n",
    "#             # 관련성이 있는 문서 추가\n",
    "#             filtered_docs.append(d)\n",
    "#         else:\n",
    "#             # 관련성이 없는 문서는 건너뛰기\n",
    "#             continue\n",
    "\n",
    "#     return {\"documents\": filtered_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc3b55",
   "metadata": {},
   "source": [
    "### 웹 검색 노드\n",
    "**외부(질문 입력)** 에서는 계속 question을 사용하고, **내부(Tavily 호출)** 에서는 query로 변환해서 넘기는 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1adf19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def web_search_node(state):\n",
    "    question = state[\"question\"]\n",
    "    res = tavily_search_tool.invoke({\"query\": question})  # dict( results=[...] )\n",
    "    items = res.get(\"results\", [])                          # list[dict]\n",
    "\n",
    "    web_results_docs = [\n",
    "        Document(\n",
    "            page_content=item.get(\"content\", \"\"),\n",
    "            metadata={\"source\": item.get(\"url\", \"\"), \"title\": item.get(\"title\", \"\")},\n",
    "        )\n",
    "        for item in items\n",
    "        if isinstance(item, dict)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    return {\"documents\": web_results_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b884d4",
   "metadata": {},
   "source": [
    "### 추가 정보 검색 필요성 여부 평가 노드(웹 검색)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f032c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_web_search_node(state):\n",
    "    filtered_docs = state[\"documents\"]\n",
    "    print(len(filtered_docs))\n",
    "    \n",
    "    if len(filtered_docs) < 20:\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        return \"rag_answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b951307",
   "metadata": {},
   "source": [
    "### 답변의 환각 여부/관련성 여부 평가 노드\n",
    "\n",
    "`groundedness_checker`: 검색과 답변을 비교\n",
    "\n",
    "`relevant_answer_checker`: 질문과 답변을 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b423f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_groundedness_check(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    \n",
    "    # Groundedness 평가: 검색과 답변 관련성 비교\n",
    "    score = groundedness_checker.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    \n",
    "    grade = score.binary_score\n",
    "    \n",
    "    # 만약 검색과 답변이 관련이 있다면,\n",
    "    # Groundedness 평가 결과에 따른 처리: 질문과 답변 관련성 비교\n",
    "    if grade == \"yes\":\n",
    "        score = relevant_answer_checker.invoke(\n",
    "            {\"question\": question, \"generation\": generation}\n",
    "        )\n",
    "        \n",
    "        grade = score.binary_score\n",
    "        \n",
    "        # 관련성 평가 결과에 따른 처리\n",
    "        if grade == \"yes\":\n",
    "            return \"relevant\"\n",
    "        else:\n",
    "            return \"not relevant\"\n",
    "        \n",
    "    else:\n",
    "        return \"not grounded\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b90ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee160568",
   "metadata": {},
   "source": [
    "## 그래프 생성\n",
    "라우팅 노드는 add_node로 추가하지 말고, `conditional_edge` 로 바로쓰자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e555ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 그래프 상태 초기화\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"query_expand\", query_rewrite_node)\n",
    "workflow.add_node(\"query_rewrite\", query_rewrite_node)\n",
    "workflow.add_node(\"retrieve\", retrieve_node)\n",
    "workflow.add_node(\"web_search\", web_search_node)\n",
    "workflow.add_node(\"rag_answer\", rag_answer_node)\n",
    "workflow.add_node(\"general_answer\", general_answer_node)\n",
    "workflow.add_node(\"grade_documents\", filtering_documents_node)\n",
    "\n",
    "\n",
    "\n",
    "# 엣지 추가\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question_node, # \n",
    "    {\n",
    "        \"query_expansion\": \"query_expand\", # 결과가 query_expansion이면 query_expand로\n",
    "        \"general_answer\": \"general_answer\" # 결과가 general_answer이면, general_answer로\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"query_expand\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_web_search_node,\n",
    "    {\n",
    "        \"web_search\": \"web_search\", # 결과가 web_search이면, web_search로\n",
    "        \"rag_answer\": \"rag_answer\" # 결과가 rag_answer이면, rag_answer로\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"rag_answer\",\n",
    "    answer_groundedness_check,\n",
    "    {\n",
    "        \"relevant\": END,\n",
    "        \"not relevant\": \"web_search\",\n",
    "        \"not grounded\": \"query_rewrite\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"query_rewrite\", \"rag_answer\")\n",
    "\n",
    "workflow.add_edge(\"web_search\", \"rag_answer\")\n",
    "\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38653196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79de795",
   "metadata": {},
   "source": [
    "## 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19f532a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36m__start__\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mquery_expand\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Can you provide an example of a relevance evaluation node used in Self-RAG within the langgraph repository?\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgrade_documents\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "2\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mrag_answer\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Self-RAG에서 사용하는 relevance evaluation 노드 예시는 다음과 같습니다. 이 노드는 사용자 질문(question)과 검색된 문서(document)의 관련성을 평가하는 역할을 합니다. LangGraph Self-RAG 구현에서 주로 사용되는 평가 템플릿과 함수 구조를 참고할 수 있습니다.\n",
      "\n",
      "아래는 Self-RAG에서 문서의 관련성을 평가하는 grader 노드 예시 코드입니다.\n",
      "\n",
      "```python\n",
      "system = \"\"\"\n",
      "You are a grader assessing relevance of a retrieved document to a user question.\n",
      "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n",
      "\"\"\"\n",
      "\n",
      "def grade_retrieval_relevance(question, doc_txt):\n",
      "    # 문서가 질문과 관련 있는지 평가\n",
      "    result = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
      "    print(result)\n",
      "    return result\n",
      "\n",
      "# 예시 사용법\n",
      "question = \"What is the capital of France?\"\n",
      "doc_txt = \"Paris is the capital city of France.\"\n",
      "\n",
      "grade_retrieval_relevance(question, doc_txt)\n",
      "```\n",
      "\n",
      "Self-RAG 파이프라인 내에서 이 노드는 다음과 같이 호출되어, 관련 문서들을 필터링하거나 재질의(query transformation)를 결정하는 데 사용됩니다.\n",
      "\n",
      "```python\n",
      "documents = retriever.invoke(question)\n",
      "generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
      "\n",
      "# 문서 관련성 평가\n",
      "for doc in documents:\n",
      "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
      "    relevance = retrieval_grader.invoke({\"question\": question, \"document\": doc})\n",
      "    if relevance == \"not relevant\":\n",
      "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
      "        # 쿼리 변환 로직 수행\n",
      "```\n",
      "\n",
      "또한, generation(LLM 응답)과 문서, 질문 간의 관련성 및 사실 근거성 평가도 함께 수행합니다.\n",
      "\n",
      "```python\n",
      "def grade_generation_v_documents_and_question(state):\n",
      "    # generation이 문서에 근거하는지, 질문에 답하는지 평가\n",
      "    print(\"---GRADE GENERATION vs QUESTION---\")\n",
      "    grade = generation_grader.invoke(state)\n",
      "    return grade\n",
      "```\n",
      "\n",
      "이처럼 Self-RAG는 문서 관련성 평가(retrieval grader), 생성 답변의 사실 근거성 평가(hallucination grader), 그리고 답변의 질문 적합성 평가(answer grader)를 조합하여, 정확하고 관련성 높은 답변만을 반환하는 폐쇄 루프 시스템을 구현합니다.\n",
      "\n",
      "요약하면, Self-RAG의 relevance evaluation 노드는 다음과 같은 형태로 구성됩니다.\n",
      "\n",
      "- `retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})`를 통해 문서-질문 관련성 평가\n",
      "- 관련 없는 문서가 많으면 쿼리 변환 수행\n",
      "- 생성 답변에 대해 hallucination grader와 answer grader를 통해 사실 근거성과 질문 적합성 평가\n",
      "\n",
      "이 예시는 LangGraph Self-RAG 구현에서 직접 참고할 수 있습니다.\n",
      "\n",
      "**Sources**  \n",
      "- Built with LangGraph! #21: Self-RAG | by Okan Yenigün - Medium  \n",
      "- Self-RAG using local LLMs - GitHub Pages  \n",
      "- Evaluate and monitor your Hybrid Search RAG | LangGraph, Qdrant miniCOIL, Opik, and DeepSeek-R1 | AI Planet  \n",
      "\n",
      "[1] https://medium.com/@okanyenigun/built-with-langgraph-21-self-rag-381ab952da6b  \n",
      "[2] https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag_local/  \n",
      "[3] https://medium.aiplanet.com/evaluate-and-monitor-your-hybrid-search-rag-langgraph-qdrant-minicoil-opik-and-deepseek-r1-a7ac70981ac3"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, random_uuid\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\n",
    "    \"question\": \"Self-RAG 에서 사용되는 관련성 평가 노드 예제를 찾아줘\",\n",
    "}\n",
    "\n",
    "# 스트리밍 형식으로 그래프 실행\n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b5ef14d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3659366440.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m--\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a04b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36m__start__\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgeneral_answer\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "대한민국의 수도는 서울입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, random_uuid\n",
    "\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "question = \"대한민국의 수도는?\"\n",
    "inputs = {\n",
    "    \"question\": question,\n",
    "}\n",
    "\n",
    "# 스트리밍 형식으로 그래프 실행\n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs,\n",
    "    config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
