{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848dbd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50776c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore, FastEmbedSparse, RetrievalMode\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "dense_embedding = OllamaEmbeddings(model=\"bge-m3\")\n",
    "sparse_embedding = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "qdrant = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"LangChain_LangGraph_QA\",\n",
    "    embedding = dense_embedding,\n",
    "    sparse_embedding = sparse_embedding,\n",
    "    retrieval_mode = RetrievalMode.HYBRID,\n",
    "    vector_name= \"dense\",\n",
    "    sparse_vector_name = \"sparse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3527d2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Self RAG\n",
      "\n",
      "Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. \n",
      "\n",
      "[Paper](https://arxiv.org/abs/2310.11511)\n",
      "\n",
      "![Screenshot 2024-04-01 at 12.41.50 PM.png](attachment:15cba0ab-a549-4909-8373-fb761e384eff.png)\n",
      "\n",
      "# Environment \n",
      "\n",
      "```python\n",
      "%pip install -qU langchain-pinecone langchain-openai langchainhub langgraph\n",
      "```\n",
      "\n",
      "### Tracing\n",
      "\n",
      "Use [LangSmith](https://docs.smith.langchain.com/) for tracing (shown at bottom)\n",
      "\n",
      "```python\n",
      "import os\n",
      "\n",
      "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
      "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
      "os.environ[\"LANGCHAIN_API_KEY\"] = \"<your-api-key>\"\n",
      "```\n",
      "\n",
      "```python\n",
      "import os\n",
      "\n",
      "os.environ[\"LANGCHAIN_PROJECT\"] = \"pinecone-devconnect\"\n",
      "```\n",
      "\n",
      "## Retriever\n",
      " \n",
      "Let's use Pinecone's sample movies database\n",
      "\n",
      "```python\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "from langchain_pinecone import PineconeVectorStore\n",
      "\n",
      "# use pinecone movies database\n",
      "\n",
      "# Add to vectorDB\n",
      "vectorstore = PineconeVectorStore(\n",
      "    embedding=OpenAIEmbeddings(),\n",
      "    index_name=\"sample-movies\",\n",
      "    text_key=\"summary\",\n",
      ")\n",
      "retriever = vectorstore.as_retriever()\n",
      "```\n",
      "\n",
      "```python\n",
      "docs = retriever.invoke(\"James Cameron\")\n",
      "for doc in docs:\n",
      "    print(\"# \" + doc.metadata[\"title\"])\n",
      "    print(doc.page_content)\n",
      "    print()\n",
      "```\n",
      "\n",
      "## Structured Output - Retrieval Grader\n",
      "\n",
      "```python\n",
      "### Retrieval Grader\n",
      "\n",
      "from langchain import hub\n",
      "from langchain_core.pydantic_v1 import BaseModel, Field\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "\n",
      "# Data model\n",
      "class GradeDocuments(BaseModel):\n",
      "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
      "\n",
      "    binary_score: str = Field(\n",
      "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
      "    )\n",
      "\n",
      "\n",
      "# https://smith.langchain.com/hub/efriis/self-rag-retrieval-grader\n",
      "grade_prompt = hub.pull(\"efriis/self-rag-retrieval-grader\")\n",
      "\n",
      "# LLM with function call\n",
      "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
      "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n"
     ]
    }
   ],
   "source": [
    "retriever = qdrant.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(retriever.invoke(\"self-rag\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a07ac",
   "metadata": {},
   "source": [
    "## RAG ì²´ì¸ êµ¬ì¶•\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” Naive í˜•íƒœì˜ Chain ì„ êµ¬ì„±í•˜ë„ë¡ í•œë‹¤.\n",
    "\n",
    "ë‹¨, Reranker ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ë„ë¡ í•œë‹¤. (`Jina-reranker` ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë‹¤ë¥¸ reranker ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ë¹¼ë„ ì¢‹ë‹¤.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be9a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import JinaRerank\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "from langchain import hub\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "# retriever ìƒì„±\n",
    "code_retriever = qdrant.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "# JinaRerank ì„¤ì •\n",
    "compressor = JinaRerank(model=\"jina-reranker-v2-base-multilingual\", top_n=8)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=code_retriever\n",
    ")\n",
    "\n",
    "# prompt ì„¤ì •\n",
    "system_prompt = \"\"\"\n",
    "You are an CODE Copilot Assistant. \n",
    "You must use the following pieces of retrieved source code or documentation to answer the question. \n",
    "You are given question related to RAG(Retrieval Augmented Generation) source code and documentation.\n",
    "If you don't know the answer, just say that you don't know. Answer in Korean.\n",
    "\n",
    "When answering questions, follow these guidelines:\n",
    "\n",
    "1. Use only the information provided in the context. \n",
    "2. Include as many example code snippets as possible.\n",
    "3. Writing a full code snippet is highly recommended.\n",
    "4. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "5. The context contain sources at the topic of each individual document.\n",
    "6. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
    "7. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
    "8. If the source is: <source>assistant/docs/llama3_1.md\" page=\"7\"</source>' then just list: \n",
    "        \n",
    "[1] llama3_1.md\n",
    "        \n",
    "And skip the addition of the brackets as well as the Document source preamble in your citation.\n",
    "\n",
    "----\n",
    "\n",
    "### Sources\n",
    "\n",
    "In the Sources section:\n",
    "- Include all sources used in your answer\n",
    "- Provide full links to relevant websites or document names\n",
    "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
    "- It will look like:\n",
    "\n",
    "**Sources**\n",
    "- [1] Link or Document name\n",
    "- [2] Link or Document name\n",
    "\n",
    "Be sure to combine sources. For example this is not correct:\n",
    "\n",
    "- [3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "- [4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "There should be no redundant sources. It should simply be:\n",
    "\n",
    "- [3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "-----\n",
    "\n",
    "### Retrieved Context\n",
    "\n",
    "Here is the context that you can use to answer the question:\n",
    "\n",
    "{context}\n",
    "\n",
    "----\n",
    "\n",
    "### Question\n",
    "\n",
    "Here is user's question:\n",
    "\n",
    "{question}\n",
    "\n",
    "----\n",
    "\n",
    "Final review:\n",
    "- Ensure the report follows the required structure\n",
    "- Check that all guidelines have been followed\n",
    "- Check if a full code snippet is included in your answer if applicable.\n",
    "- Your response should be written in Korean\n",
    "- Using many example code snippets would be rewarded by the user\n",
    "- Think step by step.\n",
    "\n",
    "----\n",
    "\n",
    "Your answer to the question with the source:\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt\n",
    "        ),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"context\": itemgetter(\"context\"),\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f98e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = rag_chain.invoke({\"question\": \"Adaptive RAG\", \"context\": compression_retriever.invoke(\"Adaptive RAG\")})\n",
    "response = rag_chain.invoke({\"question\": \"Adaptive RAG\", \"context\": retriever.invoke(\"Adaptive RAG\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88365496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive RAGëŠ” RAG(Retrieval Augmented Generation) ì „ëµ ì¤‘ í•˜ë‚˜ë¡œ, ì¿¼ë¦¬ ë¶„ì„(query analysis)ê³¼ ëŠ¥ë™ì (self-corrective) RAGë¥¼ ê²°í•©í•œ ë°©ì‹ì…ë‹ˆë‹¤. ì´ ì „ëµì€ ì¿¼ë¦¬ì˜ íŠ¹ì„±ì— ë”°ë¼ ì ì ˆí•œ ì²˜ë¦¬ ê²½ë¡œë¥¼ ì„ íƒí•˜ì—¬ ë” íš¨ìœ¨ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\n",
      "\n",
      "### Adaptive RAG ì£¼ìš” ê°œë…\n",
      "- ì¿¼ë¦¬ ë¶„ì„ì„ í†µí•´ ì§ˆë¬¸ ìœ í˜•ì„ íŒŒì•…í•˜ê³ , ì´ì— ë”°ë¼ ë‹¤ìŒ ê²½ë¡œ ì¤‘ í•˜ë‚˜ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤:\n",
      "  - Retrieval ì—†ì´ LLMì´ ì§ì ‘ ë‹µë³€í•˜ëŠ” ê²½ìš° (No Retrieval)\n",
      "  - ë‹¨ì¼ ê²€ìƒ‰ í›„ ë‹µë³€ ìƒì„± (Single-shot RAG)\n",
      "  - ë°˜ë³µì  ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± (Iterative RAG)\n",
      "  - ì›¹ ê²€ìƒ‰(Web-search)ì„ í†µí•œ ë³´ì™„\n",
      "- ì˜ˆë¥¼ ë“¤ì–´, ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ ì§ˆë¬¸ì€ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ, ì¸ë±ìŠ¤ ë‚´ ì •ë³´ê°€ ì¶©ë¶„í•œ ì§ˆë¬¸ì€ RAGë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "### Adaptive RAG êµ¬í˜„ ì˜ˆì‹œ (LangGraph ê¸°ë°˜)\n",
      "\n",
      "```python\n",
      "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
      "! pip install --quiet langchain langchain_cohere langchain-openai tiktoken langchainhub chromadb langgraph\n",
      "\n",
      "import os\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain_cohere import CohereEmbeddings\n",
      "from langchain_community.document_loaders import WebBaseLoader\n",
      "from langchain_community.vectorstores import Chroma\n",
      "\n",
      "# API í‚¤ ì„¤ì •\n",
      "os.environ[\"COHERE_API_KEY\"] = \"<your-cohere-api-key>\"\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"<your-openai-api-key>\"\n",
      "\n",
      "# ë¬¸ì„œ URL ë¦¬ìŠ¤íŠ¸\n",
      "urls = [\n",
      "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
      "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
      "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
      "]\n",
      "\n",
      "# ì›¹ì—ì„œ ë¬¸ì„œ ë¡œë“œ\n",
      "docs = [WebBaseLoader(url).load() for url in urls]\n",
      "docs_list = [item for sublist in docs for item in sublist]\n",
      "\n",
      "# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •\n",
      "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
      "split_docs = text_splitter.split_documents(docs_list)\n",
      "\n",
      "# ì„ë² ë”© ìƒì„±ê¸° ì„¤ì •\n",
      "embeddings = CohereEmbeddings()\n",
      "\n",
      "# ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (Chroma ì‚¬ìš©)\n",
      "vectorstore = Chroma.from_documents(split_docs, embeddings)\n",
      "\n",
      "# Adaptive RAGì˜ ì¿¼ë¦¬ ë¶„ì„ ë° ë¼ìš°íŒ… ë¡œì§ ì˜ˆì‹œ (ì˜ì‚¬ ì½”ë“œ)\n",
      "def adaptive_rag(query: str):\n",
      "    # 1. ì¿¼ë¦¬ ë¶„ì„ (ì˜ˆ: ìµœì‹  ì •ë³´ í•„ìš” ì—¬ë¶€ íŒë‹¨)\n",
      "    if \"ìµœì‹ \" in query or \"ìµœê·¼\" in query:\n",
      "        # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰\n",
      "        print(\"ì›¹ ê²€ìƒ‰ ê²½ë¡œ ì„ íƒ\")\n",
      "        # ì›¹ ê²€ìƒ‰ API í˜¸ì¶œ ë° ê²°ê³¼ ì²˜ë¦¬\n",
      "    else:\n",
      "        # ì¸ë±ìŠ¤ ë‚´ ê²€ìƒ‰ ìˆ˜í–‰\n",
      "        print(\"RAG ê²½ë¡œ ì„ íƒ\")\n",
      "        # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ í›„ LLMì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±\n",
      "\n",
      "# ì˜ˆì‹œ ì¿¼ë¦¬\n",
      "adaptive_rag(\"ìµœì‹  AI ì—°êµ¬ ë™í–¥ì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
      "adaptive_rag(\"í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
      "```\n",
      "\n",
      "### ìš”ì•½\n",
      "- Adaptive RAGëŠ” ì¿¼ë¦¬ ë¶„ì„ì„ í†µí•´ ì ì ˆí•œ ê²€ìƒ‰ ë° ìƒì„± ì „ëµì„ ì„ íƒí•˜ëŠ” RAG ë°©ì‹ì…ë‹ˆë‹¤.\n",
      "- ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° ì›¹ ê²€ìƒ‰ì„, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° ì¸ë±ìŠ¤ ê¸°ë°˜ RAGë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "- LangGraphì™€ Cohere Embeddings, Chroma ë²¡í„° ìŠ¤í† ì–´ ë“±ì„ í™œìš©í•´ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë” ìì„¸í•œ ë‚´ìš©ê³¼ êµ¬í˜„ ì˜ˆì‹œëŠ” ì•„ë˜ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì„¸ìš”.\n",
      "\n",
      "**Sources**\n",
      "- Adaptive RAG Cohere Command R ì˜ˆì œ: langgraph_adaptive_rag_cohere.ipynb  \n",
      "- Adaptive RAG ê¸°ë³¸ ê°œë… ë° êµ¬í˜„: langgraph_adaptive_rag.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70141a6c",
   "metadata": {},
   "source": [
    "## LangGraph ì²´ì¸ êµ¬ì¶•ì„ ìœ„í•œ ë…¸ë“œ ì •ì˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422f6b2",
   "metadata": {},
   "source": [
    "### Routing\n",
    "\n",
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ routing ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì§ˆë¬¸ì´ vector ê²€ìƒ‰ì´ í•„ìš”í•œì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì—¬ ë¼ìš°íŒ…í•˜ë©°, ìµœëŒ€í•œ ë³´ìˆ˜ì ìœ¼ë¡œ íŒë‹¨í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ì •í•˜ì˜€ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4769684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question, determine if it needs to be retrieved from vectorstore or not. Return 'yes' if it needs to be retrieved from vectorstore, otherwise return 'no'.\",\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system = \"\"\"\n",
    "You are an expert at routing a user question. \n",
    "The vectorstore contains documents retrieved using LangChain / LangGraph.\n",
    "The vectorstore contains documents related to RAG(Retrieval Augmented Generation) source code and documentation.\n",
    "Return 'yes' if the question is related to the source code or documentation, otherwise return 'no'.\n",
    "If you can't determine if the question is related to the source code or documentation, return 'yes'.\n",
    "If you don't know the answer, return 'yes'.\n",
    "\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbd4c0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(binary_score='yes')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_router.invoke({\"question\": \"MemorySaver ì— ëŒ€í•´ ì„¤ëª…í•˜ì„¸ìš”.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df101d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(binary_score='no')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_router.invoke({\"question\": \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b606f",
   "metadata": {},
   "source": [
    "## Query Rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be4468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "\n",
    "system = \"\"\"\n",
    "You a question re-writer that converts an input question to a better version that is optimized for CODE SEARCH(github repository).\n",
    "\n",
    "Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "\n",
    "Base Code Repository: \n",
    "\n",
    "https://github.com/langchain-ai/langgraph\n",
    "\n",
    "Output should be in English.\"\"\"\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bc6d520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I implement Hierarchical Retrieval-Augmented Generation (RAG) using the LangGraph framework?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_rewriter.invoke(\"Hierarchical RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35452669",
   "metadata": {},
   "source": [
    "## í‰ê°€\n",
    "\n",
    "- ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„± í‰ê°€\n",
    "- ë‹µë³€ì˜ í™˜ê° ì—¬ë¶€ í‰ê°€\n",
    "- ë‹µë³€ì˜ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ì„± í‰ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96610d",
   "metadata": {},
   "source": [
    "### ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„± í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "784dab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    \n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc3fe4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"self-rag\"\n",
    "# retrieval_grader.invoke({\"question\": question, \"document\": retriever.invoke(question)[0].page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99bf18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel, Field\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# # ë¬¸ì„œ í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„° ëª¨ë¸ ì •ì˜\n",
    "# class GradeDocuments(BaseModel):\n",
    "#     \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "#     binary_score: str = Field(\n",
    "#         description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# # LLM ì´ˆê¸°í™” ë° í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±\n",
    "# structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "# system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "#     If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "#     It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "#     Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "# grade_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system),\n",
    "#         (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # ë¬¸ì„œ ê²€ìƒ‰ê²°ê³¼ í‰ê°€ê¸° ìƒì„±\n",
    "# retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6605e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"self-rag\"\n",
    "# retrieval_grader.invoke({\"question\": question, \"document\": compression_retriever.invoke(question)[0].page_content})\n",
    "\n",
    "# question = \"self-rag\"\n",
    "# retrieval_grader.invoke({\"question\": question, \"document\": retriever.invoke(question)[0].page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0859d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e238c81e",
   "metadata": {},
   "source": [
    "### ë‹µë³€ì˜ í™˜ê° ì—¬ë¶€ í‰ê°€\n",
    "\n",
    "`groundedness_checker`: ê²€ìƒ‰ê³¼ ë‹µë³€ì„ ë¹„êµ\n",
    "\n",
    "`relevant_answer_checker`: ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e89b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerGroundedness(BaseModel):\n",
    "    \"\"\"Binary score for answer groundedness\"\"\"\n",
    "    \n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is groundedness in the facts(given context), 'yes' or 'no'\"\n",
    "    )\n",
    "    \n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)    \n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(AnswerGroundedness)\n",
    "\n",
    "system = \"\"\"\n",
    "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\n",
    "\"\"\"\n",
    "\n",
    "groundedness_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ë‹µë³€ì˜ í™˜ê° ì—¬ë¶€ í‰ê°€ê¸° ìƒì„±\n",
    "groundedness_checker = groundedness_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "101a6109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerGroundedness(binary_score='yes')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question = \"self-rag\"\n",
    "# groundedness_checker.invoke({\"documents\": compression_retriever.invoke(question), \"generation\": rag_chain.invoke({\"question\": question, \"context\": compression_retriever.invoke(question)})})\n",
    "\n",
    "question = \"self-rag\"\n",
    "groundedness_checker.invoke({\"documents\": retriever.invoke(question), \"generation\": rag_chain.invoke({\"question\": question, \"context\": retriever.invoke(question)})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dde46c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary scoring to evaluate the appropriateness of answers to questions\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicate 'yes' or 'no' whether the answer solves the question\"\n",
    "    )\n",
    "\n",
    "\n",
    "# í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•œ LLM ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "relevant_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ êµ¬ì¡°í™”ëœ LLM í‰ê°€ê¸°ë¥¼ ê²°í•©í•˜ì—¬ ë‹µë³€ í‰ê°€ê¸° ìƒì„±\n",
    "relevant_answer_checker = relevant_answer_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c544aeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question = \"self-rag\"\n",
    "# relevant_answer_checker.invoke({\"question\": question, \n",
    "#                                 \"generation\": rag_chain.invoke({\"question\": question, \"context\": qdrant.invoke(question)[0].page_content})})\n",
    "\n",
    "question = \"self-rag\"\n",
    "relevant_answer_checker.invoke({\"question\": question, \n",
    "                                \"generation\": rag_chain.invoke({\"question\": question, \"context\": retriever.invoke(question)[0].page_content})})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1395f42",
   "metadata": {},
   "source": [
    "## ë„êµ¬ì •ì˜\n",
    "\n",
    "ì›¹ì„œì¹˜ë¥¼ ìœ„í•œ TavilySearch \n",
    "\n",
    "DUCKDUCKGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2cd5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily.tavily_search import TavilySearch\n",
    "\n",
    "tavily_search_tool = TavilySearch(max_result=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93eb7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tavily_search_tool.invoke(\"agent2agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6be860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
    "\n",
    "# duck = DuckDuckGoSearchRun(max_result=3)\n",
    "# print(duck.invoke(\"agent2agent\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9b0cc",
   "metadata": {},
   "source": [
    "## ìƒíƒœì •ì˜\n",
    "\n",
    "`AgentState` :  `messages: Annotated[Sequence[BaseMessage], add_messages]`ë¡œ ì •ì˜í•œ messagesê°€ ì¶”ê°€ëœ í˜•íƒœ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8cc32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Annotated\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "\n",
    "# class GraphState(TypedDict):\n",
    "#     question: Annotated[str, \"User question\"]\n",
    "#     gneration: Annotated[str, \"LLM generated answer\"]\n",
    "#     documents: Annotated[list[Document], \"List of documents\"]\n",
    "\n",
    "class GraphState(AgentState):\n",
    "    question: Annotated[str, \"User question\"]\n",
    "    generation: Annotated[str, \"LLM generated answer\"]\n",
    "    documents: Annotated[list[Document], \"List of documents\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60139064",
   "metadata": {},
   "source": [
    "## ë…¸ë“œ ì •ì˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7c9d6",
   "metadata": {},
   "source": [
    "### ì§ˆë¬¸ ë¼ìš°íŒ… ë…¸ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8351c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question_node(state):\n",
    "    question = state[\"question\"]\n",
    "    evaluation = question_router.invoke({\"question\": question})\n",
    "    \n",
    "    if evaluation.binary_score == \"yes\":\n",
    "        return \"query_expansion\"\n",
    "    else: \n",
    "        return \"general_answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ffa5b",
   "metadata": {},
   "source": [
    "### ì§ˆë¬¸ ì¬ì‘ì„± ë…¸ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a18924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rewrite_node(state):\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    \n",
    "    return {\"question\": better_question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed1469",
   "metadata": {},
   "source": [
    "### ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60ee3eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_node(state):\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # documents = compression_retriever.invoke(question)\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a695da",
   "metadata": {},
   "source": [
    "### ì¼ë°˜ ë‹µë³€ ìƒì„± ë…¸ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbdd9068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "def general_answer_node(state):\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "    question = state[\"question\"]\n",
    "    answer = llm.invoke(question)\n",
    "    return {\"generation\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8696fd",
   "metadata": {},
   "source": [
    "### RAG ë‹µë³€ ìƒì„± ë…¸ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "832695b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer_node(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    answer = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"generation\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eea462",
   "metadata": {},
   "source": [
    "### ë¬¸ì„œ ê´€ë ¨ì„±ì„ í‰ê°€ í›„ í•„í„°ë§\n",
    "\n",
    "filtered_docsì— ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c4f1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_documents_node(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    filtered_docs = []\n",
    "    \n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return {\"documents\": filtered_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19bf1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€ í›„ í•„í„°ë§\n",
    "# def filtering_documents_node(state):\n",
    "#     # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "#     question = state[\"question\"]\n",
    "#     documents = state[\"documents\"]\n",
    "\n",
    "#     # ê° ë¬¸ì„œì— ëŒ€í•œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°\n",
    "#     filtered_docs = []\n",
    "#     for d in documents:\n",
    "#         score = retrieval_grader.invoke(\n",
    "#             {\"question\": question, \"document\": d.page_content}\n",
    "#         )\n",
    "#         grade = score.binary_score\n",
    "#         if grade == \"yes\":\n",
    "#             # ê´€ë ¨ì„±ì´ ìˆëŠ” ë¬¸ì„œ ì¶”ê°€\n",
    "#             filtered_docs.append(d)\n",
    "#         else:\n",
    "#             # ê´€ë ¨ì„±ì´ ì—†ëŠ” ë¬¸ì„œëŠ” ê±´ë„ˆë›°ê¸°\n",
    "#             continue\n",
    "\n",
    "#     return {\"documents\": filtered_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc3b55",
   "metadata": {},
   "source": [
    "### ì›¹ ê²€ìƒ‰ ë…¸ë“œ\n",
    "**ì™¸ë¶€(ì§ˆë¬¸ ì…ë ¥)** ì—ì„œëŠ” ê³„ì† questionì„ ì‚¬ìš©í•˜ê³ , **ë‚´ë¶€(Tavily í˜¸ì¶œ)** ì—ì„œëŠ” queryë¡œ ë³€í™˜í•´ì„œ ë„˜ê¸°ëŠ” êµ¬ì¡°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1adf19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def web_search_node(state):\n",
    "    question = state[\"question\"]\n",
    "    res = tavily_search_tool.invoke({\"query\": question})  # dict( results=[...] )\n",
    "    items = res.get(\"results\", [])                          # list[dict]\n",
    "\n",
    "    web_results_docs = [\n",
    "        Document(\n",
    "            page_content=item.get(\"content\", \"\"),\n",
    "            metadata={\"source\": item.get(\"url\", \"\"), \"title\": item.get(\"title\", \"\")},\n",
    "        )\n",
    "        for item in items\n",
    "        if isinstance(item, dict)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    return {\"documents\": web_results_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b884d4",
   "metadata": {},
   "source": [
    "### ì¶”ê°€ ì •ë³´ ê²€ìƒ‰ í•„ìš”ì„± ì—¬ë¶€ í‰ê°€ ë…¸ë“œ(ì›¹ ê²€ìƒ‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f032c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_web_search_node(state):\n",
    "    filtered_docs = state[\"documents\"]\n",
    "    print(len(filtered_docs))\n",
    "    \n",
    "    if len(filtered_docs) < 20:\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        return \"rag_answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b951307",
   "metadata": {},
   "source": [
    "### ë‹µë³€ì˜ í™˜ê° ì—¬ë¶€/ê´€ë ¨ì„± ì—¬ë¶€ í‰ê°€ ë…¸ë“œ\n",
    "\n",
    "`groundedness_checker`: ê²€ìƒ‰ê³¼ ë‹µë³€ì„ ë¹„êµ\n",
    "\n",
    "`relevant_answer_checker`: ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b423f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_groundedness_check(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    \n",
    "    # Groundedness í‰ê°€: ê²€ìƒ‰ê³¼ ë‹µë³€ ê´€ë ¨ì„± ë¹„êµ\n",
    "    score = groundedness_checker.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    \n",
    "    grade = score.binary_score\n",
    "    \n",
    "    # ë§Œì•½ ê²€ìƒ‰ê³¼ ë‹µë³€ì´ ê´€ë ¨ì´ ìˆë‹¤ë©´,\n",
    "    # Groundedness í‰ê°€ ê²°ê³¼ì— ë”°ë¥¸ ì²˜ë¦¬: ì§ˆë¬¸ê³¼ ë‹µë³€ ê´€ë ¨ì„± ë¹„êµ\n",
    "    if grade == \"yes\":\n",
    "        score = relevant_answer_checker.invoke(\n",
    "            {\"question\": question, \"generation\": generation}\n",
    "        )\n",
    "        \n",
    "        grade = score.binary_score\n",
    "        \n",
    "        # ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼ì— ë”°ë¥¸ ì²˜ë¦¬\n",
    "        if grade == \"yes\":\n",
    "            return \"relevant\"\n",
    "        else:\n",
    "            return \"not relevant\"\n",
    "        \n",
    "    else:\n",
    "        return \"not grounded\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b90ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee160568",
   "metadata": {},
   "source": [
    "## ê·¸ë˜í”„ ìƒì„±\n",
    "ë¼ìš°íŒ… ë…¸ë“œëŠ” add_nodeë¡œ ì¶”ê°€í•˜ì§€ ë§ê³ , `conditional_edge` ë¡œ ë°”ë¡œì“°ì!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e555ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒíƒœ ì´ˆê¸°í™”\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ë…¸ë“œ ì •ì˜\n",
    "workflow.add_node(\"query_expand\", query_rewrite_node)\n",
    "workflow.add_node(\"query_rewrite\", query_rewrite_node)\n",
    "workflow.add_node(\"retrieve\", retrieve_node)\n",
    "workflow.add_node(\"web_search\", web_search_node)\n",
    "workflow.add_node(\"rag_answer\", rag_answer_node)\n",
    "workflow.add_node(\"general_answer\", general_answer_node)\n",
    "workflow.add_node(\"grade_documents\", filtering_documents_node)\n",
    "\n",
    "\n",
    "\n",
    "# ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question_node, # \n",
    "    {\n",
    "        \"query_expansion\": \"query_expand\", # ê²°ê³¼ê°€ query_expansionì´ë©´ query_expandë¡œ\n",
    "        \"general_answer\": \"general_answer\" # ê²°ê³¼ê°€ general_answerì´ë©´, general_answerë¡œ\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"query_expand\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_web_search_node,\n",
    "    {\n",
    "        \"web_search\": \"web_search\", # ê²°ê³¼ê°€ web_searchì´ë©´, web_searchë¡œ\n",
    "        \"rag_answer\": \"rag_answer\" # ê²°ê³¼ê°€ rag_answerì´ë©´, rag_answerë¡œ\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"rag_answer\",\n",
    "    answer_groundedness_check,\n",
    "    {\n",
    "        \"relevant\": END,\n",
    "        \"not relevant\": \"web_search\",\n",
    "        \"not grounded\": \"query_rewrite\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"query_rewrite\", \"rag_answer\")\n",
    "\n",
    "workflow.add_edge(\"web_search\", \"rag_answer\")\n",
    "\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38653196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79de795",
   "metadata": {},
   "source": [
    "## ê·¸ë˜í”„ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19f532a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36m__start__\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mquery_expand\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Can you provide an example of a relevance evaluation node used in Self-RAG within the langgraph repository?\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgrade_documents\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "2\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mrag_answer\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Self-RAGì—ì„œ ì‚¬ìš©í•˜ëŠ” relevance evaluation ë…¸ë“œ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ ë…¸ë“œëŠ” ì‚¬ìš©ì ì§ˆë¬¸(question)ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œ(document)ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. LangGraph Self-RAG êµ¬í˜„ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” í‰ê°€ í…œí”Œë¦¿ê³¼ í•¨ìˆ˜ êµ¬ì¡°ë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì•„ë˜ëŠ” Self-RAGì—ì„œ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” grader ë…¸ë“œ ì˜ˆì‹œ ì½”ë“œì…ë‹ˆë‹¤.\n",
      "\n",
      "```python\n",
      "system = \"\"\"\n",
      "You are a grader assessing relevance of a retrieved document to a user question.\n",
      "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n",
      "\"\"\"\n",
      "\n",
      "def grade_retrieval_relevance(question, doc_txt):\n",
      "    # ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ì§€ í‰ê°€\n",
      "    result = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
      "    print(result)\n",
      "    return result\n",
      "\n",
      "# ì˜ˆì‹œ ì‚¬ìš©ë²•\n",
      "question = \"What is the capital of France?\"\n",
      "doc_txt = \"Paris is the capital city of France.\"\n",
      "\n",
      "grade_retrieval_relevance(question, doc_txt)\n",
      "```\n",
      "\n",
      "Self-RAG íŒŒì´í”„ë¼ì¸ ë‚´ì—ì„œ ì´ ë…¸ë“œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í˜¸ì¶œë˜ì–´, ê´€ë ¨ ë¬¸ì„œë“¤ì„ í•„í„°ë§í•˜ê±°ë‚˜ ì¬ì§ˆì˜(query transformation)ë¥¼ ê²°ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "```python\n",
      "documents = retriever.invoke(question)\n",
      "generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
      "\n",
      "# ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€\n",
      "for doc in documents:\n",
      "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
      "    relevance = retrieval_grader.invoke({\"question\": question, \"document\": doc})\n",
      "    if relevance == \"not relevant\":\n",
      "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
      "        # ì¿¼ë¦¬ ë³€í™˜ ë¡œì§ ìˆ˜í–‰\n",
      "```\n",
      "\n",
      "ë˜í•œ, generation(LLM ì‘ë‹µ)ê³¼ ë¬¸ì„œ, ì§ˆë¬¸ ê°„ì˜ ê´€ë ¨ì„± ë° ì‚¬ì‹¤ ê·¼ê±°ì„± í‰ê°€ë„ í•¨ê»˜ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "\n",
      "```python\n",
      "def grade_generation_v_documents_and_question(state):\n",
      "    # generationì´ ë¬¸ì„œì— ê·¼ê±°í•˜ëŠ”ì§€, ì§ˆë¬¸ì— ë‹µí•˜ëŠ”ì§€ í‰ê°€\n",
      "    print(\"---GRADE GENERATION vs QUESTION---\")\n",
      "    grade = generation_grader.invoke(state)\n",
      "    return grade\n",
      "```\n",
      "\n",
      "ì´ì²˜ëŸ¼ Self-RAGëŠ” ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€(retrieval grader), ìƒì„± ë‹µë³€ì˜ ì‚¬ì‹¤ ê·¼ê±°ì„± í‰ê°€(hallucination grader), ê·¸ë¦¬ê³  ë‹µë³€ì˜ ì§ˆë¬¸ ì í•©ì„± í‰ê°€(answer grader)ë¥¼ ì¡°í•©í•˜ì—¬, ì •í™•í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ë‹µë³€ë§Œì„ ë°˜í™˜í•˜ëŠ” íì‡„ ë£¨í”„ ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
      "\n",
      "ìš”ì•½í•˜ë©´, Self-RAGì˜ relevance evaluation ë…¸ë“œëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.\n",
      "\n",
      "- `retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})`ë¥¼ í†µí•´ ë¬¸ì„œ-ì§ˆë¬¸ ê´€ë ¨ì„± í‰ê°€\n",
      "- ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œê°€ ë§ìœ¼ë©´ ì¿¼ë¦¬ ë³€í™˜ ìˆ˜í–‰\n",
      "- ìƒì„± ë‹µë³€ì— ëŒ€í•´ hallucination graderì™€ answer graderë¥¼ í†µí•´ ì‚¬ì‹¤ ê·¼ê±°ì„±ê³¼ ì§ˆë¬¸ ì í•©ì„± í‰ê°€\n",
      "\n",
      "ì´ ì˜ˆì‹œëŠ” LangGraph Self-RAG êµ¬í˜„ì—ì„œ ì§ì ‘ ì°¸ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**Sources**  \n",
      "- Built with LangGraph! #21: Self-RAG | by Okan YenigÃ¼n - Medium  \n",
      "- Self-RAG using local LLMs - GitHub Pages  \n",
      "- Evaluate and monitor your Hybrid Search RAG | LangGraph, Qdrant miniCOIL, Opik, and DeepSeek-R1 | AI Planet  \n",
      "\n",
      "[1] https://medium.com/@okanyenigun/built-with-langgraph-21-self-rag-381ab952da6b  \n",
      "[2] https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag_local/  \n",
      "[3] https://medium.aiplanet.com/evaluate-and-monitor-your-hybrid-search-rag-langgraph-qdrant-minicoil-opik-and-deepseek-r1-a7ac70981ac3"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, random_uuid\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"question\": \"Self-RAG ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê´€ë ¨ì„± í‰ê°€ ë…¸ë“œ ì˜ˆì œë¥¼ ì°¾ì•„ì¤˜\",\n",
    "}\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹ìœ¼ë¡œ ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b5ef14d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3659366440.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m--\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a04b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36m__start__\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgeneral_answer\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, random_uuid\n",
    "\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "question = \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?\"\n",
    "inputs = {\n",
    "    \"question\": question,\n",
    "}\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹ìœ¼ë¡œ ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs,\n",
    "    config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
